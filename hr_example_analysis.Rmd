---
title: "Example HR research using interactive R Markdown"
date: "February 4, 2019"
author: "Jeff Idle"
output: html_notebook
runtime: shiny
editor_options: 
  chunk_output_type: inline
---

# Purpose and Background
The purpose of this analysis is to showcase the capabilities of interactive R Markdown documents. R Markdown documents are great for integrating analysis with visualizations. Interactive R Markdown documents provide a better way to engage your stakeholders and let them explore your analysis further. IBM posted a sample Human Resources Employee Attrition and Performance dataset on their [Watson Analytics Community web site](https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/). The imaginary stakeholders for this example analysis will be the CEO and HR VP for a fictional medium-sized company. Our fictional company researches and designs productivity and entertainment peripherals that can be biologically integrated with the human body and connect to the IoT (Internet of Things). Our fictional company is called "Shadowrun, Inc.". The dataset does not contain hire dates or termination dates, so for purposes of this analysis we will assume all of the attrition in our dataset represents voluntary turnover for the last five years. The source code for this analysis can be found on Github at https://github.com/jeffidle/hr_attrition_example.

**NOTE:**  Please note that this analysis is an example for illustrative purposes only. As such, this analysis is not guaranteed to be error-free or copied without further analysis and modifications to fit the situation to which it is being applied. 


# Business Questions
Our analytics team met with Shadow Run's CEO and HR VP to discuss their critical workforce planning and management questions. They identified and prioritized research into the following three questions:

  1. At what levels of service with the company do we have the greatest risk of losing employees?
  2. What are the key drivers of Shadow Run's turnover?
  3. Can we develop a model to predict the risk of losing our currently active employees?


# Executive Summary
Our analytics team found the following:

  * **High risk levels of service >>** The years in which we have the highest risks of losing our employees are in years 1 and 10. Further research would need to be conducted to determine the underlying root causes. Intuitively, the root causes could be standard new hire issues like misunderstanding of what the job would be or poor fit with company culture. The year 10 drop in retention is particularly interesting because intuitive explanations do not readily come to mind.

  * **Voluntary turnover drivers >>** TBD

  * **Turnover risk model >>** I tested ten models and recommend that we operationalize a feature-selected and cross-validated logistic regression model for our retention risk modeling. This model was the most accurate and provides the additional benefit of being able to explain factors that appear to be correlated with the additional risk. The accuracy of the model is expected to be in the range of X% to X% with a mean accuracy of X%.


# Methodologies
I utilized several methodologies in answering the business questions. Following are the methods we applied for each business question. Following is a summary of the analytic methodologies I used in addition to descriptive analysis.

  * **Retention risk by years of service >>** Survival analysis
  * **Voluntary turnover drivers >>** Random forest & logistic regression
  * **Retention risk model >>** Machine learning, logistic regression, naive Bayes, random forest, regression trees & C5.0 decision trees.


# Analysis
The analysis that follows will start with exploratory analyses to understand the data. We will then analyze the data to answer the question about years of service where retention risk is greatest. Next we will analyze the key drivers that are highly correlated with turnover during the last 5 years. Finally, we will create a predictive model that enables us to assess the risk of losing our currently active employees and estimate the risk of losing any new employees that join the organization.


#### Exploratory analysis
The starting point for every analysis is to become familiar with the data. You can view the raw data [here](https://github.com/jeffidle/hr_attrition_example/blob/master/data/ibm_attrition.csv) but let's take a look at a summarized view of the data. 

```{r echo = FALSE, message=FALSE, warning=FALSE}

source("hr_example_source_code.R")

```

```{r echo = FALSE, message=FALSE, warning=FALSE}

data_summary_df %>% gt() %>%
        tab_options(table.font.size = 10,
                    row.padding = px(1)
                    )

```
<br>
<br>
Let's visualize the data. We would first like to see how are data is distributed among active and terminated records. The following chart shows that 16% of the records in our dataset are voluntary terminations and the remaining 84% are currently active employees.

```{r echo = FALSE, message=FALSE, warning=FALSE}

renderPlot({
  term_dist_viz
})

```
<br>

The drop-down below allows you to view the counts of employees by the selected dimension and the distribution of active versus terminated employee records within the levels of that dimension. Feel free to use the drop-down to explore the data further.

```{r echo = FALSE}

selectInput(inputId = "explore_dim", label = "Choose the variable that you would like to explore:",
            choices = c("Business Travel", "Department", "Education", "Education Field", "Gender", "Job Involvement", "Job Role",
                        "Job Satisfaction", "Marital Status", "Overtime", "Peformance Rating", "Relationship Satisfaction",
                        "Stock Options Level", "Work/Life Balance"), selected = "Gender")

dim1_select <- reactive({
        switch(input$explore_dim,
               "Business Travel" = attrition_df$BusinessTravel,
               "Department" = attrition_df$Department,
               "Education" = attrition_df$EducationText,
               "Education Field" = attrition_df$EducationField,
               "Gender" = attrition_df$Gender,
               "Job Involvement" = attrition_df$JobInvolvementText,
               "Job Role" = attrition_df$JobRole,
               "Job Satisfaction" = attrition_df$JobSatisfactionText,
               "Marital Status" = attrition_df$MaritalStatus,
               "Overtime" = attrition_df$OverTime,
               "Performance Rating" = attrition_df$PerformanceRatingText,
               "Stock Options Level" = attrition_df$StockOptionLevel,
               "Work/Life Balance" = attrition_df$WorkLifeBalanceText)
        })

renderPlot({
        
        explore_dist_viz <- ggplot(attrition_df, aes(x = dim1_select())) +
                geom_bar(fill = "#8f8f8f", width = 0.5) +
                geom_text(stat = "count", aes(label = ..count..), hjust = 1.5, color = "#ffffff") +
                labs(title = paste0("Employee count distribution"), x = "", y = "") +
                coord_flip() +
                theme(panel.background = element_blank(),
                      axis.line.x = element_blank(),
                      axis.line.y = element_blank(),
                      axis.ticks.x = element_blank(),
                      axis.ticks.y = element_blank(),
                      axis.text.x = element_blank(),
                      legend.position = "none")
        
        explore2_dist_viz <- ggplot(attrition_df, aes(x = dim1_select(), y = EmployeeCount, fill = Attrition)) +
                geom_bar(position = "fill", stat = "identity", width = 0.5) +
                labs(title = paste0("Active versus terminated distribution"), x = "", y = "") +
                scale_fill_manual(values = c("#ED8800", "#5378b5")) +
                scale_y_continuous(label = percent_format()) +
                coord_flip() +
                theme(panel.background = element_blank(),
                      axis.line.y = element_blank(),
                      axis.ticks.y = element_blank())
        
        explore2_dist_viz
        
        grid.arrange(explore_dist_viz, explore2_dist_viz, ncol = 2)
        
})

```


#### Retention Risk by Years of Service
Understanding whether there are years throughout an employee's tenure in which retention risk is highest can help Shadowrun create focused and targeted retention programs. Targeted programs should be more effective than one-size-fits-all programs because targeted programs are tailored to the specific needs of the population at risk. By leveraging the survival analysis methodology in the context of employee attrition, we can assess the average drop in retention probability for each year of service to identify those years in which retention risk is greatest.

The chart on the left shows how the probability of retaining Shadowrun employees drops as tenure increases. This is expected to some extent--everyone will retire someday and that is part of voluntary termination. If all Shadowrun employees were hired directly from high school or college and stayed until they retired, then the "survival curve" would bow outward to the right. If employees were leaving within relatively short periods of time, then the "survival curve" would bow inward to the left. Shadowrun's curve bows outward to the right, which tells us that our employees tend to stay with us. In fact, our employees have a 25% probability of staying with us for 15 years and 40% probability of staying with us for 31 years. The first half of the curve is generally smooth, which tells us there isn't a lot of year-over-year variation. The jagged portions at the end of the curve exist simply because there is a very small population of employees at those tenure levels. 

The chart on the right shows the average reduction in retention probability for each year of service. This chart focuses on the first 20 years since that constitutes the bulk of our workforce. The overall average decrease in retention probability for each year of service is 1.4% if we exclude the outlier in year 40. We can see in years 1 and 10 that the decrease is around 4%--nearly 3x the average. We should work with Shadowrun's HR Business Partners to better understand what might be going on in these years. The first year spike is not unexpected and may be due to onboarding, poor culutural fit, or other factors common to employees accepting a new job at a new company. The tenth-year spike, however, is unusual and should be investigated further.

```{r echo = FALSE}

renderPlot({
  grid.arrange(survival_viz, risk_viz, ncol = 2)
})

```
<br>
<br>
Overall, we don't have major concerns about retention risk by tenure, but we should investigate the spikes in years 1 and 10. We would be surprised if there is not a policy, program or issue that explains why more employees leave in their 10th year than normal. What we don't know, however, is whether there are segments of the population where there are more clear patterns in higher retention risk by tenure. Use the drop-down below to explore various dimensions. If you slice by Education Field you will notice the employees whose education is in Human Resources have a high retention risk increases in years 2 - 4.

```{r echo = FALSE}

selectInput(inputId = "survival_slice", label = "Choose the dimension by which you'd like to slice the retention analysis:", 
  choices = c("Business Travel", "Department", "Education", "Education Field", "Gender", "Job Involvement", "Job Role",
                        "Job Satisfaction", "Marital Status", "Overtime", "Peformance Rating", "Relationship Satisfaction",
                        "Stock Options Level", "Work/Life Balance"), selected = "Gender")

dimension_choice <- reactive({
        switch(input$survival_slice,
               "Business Travel" = attrition_df$BusinessTravel,
               "Department" = attrition_df$Department,
               "Education" = attrition_df$EducationText,
               "Education Field" = attrition_df$EducationField,
               "Gender" = attrition_df$Gender,
               "Job Involvement" = attrition_df$JobInvolvementText,
               "Job Role" = attrition_df$JobRole,
               "Job Satisfaction" = attrition_df$JobSatisfactionText,
               "Marital Status" = attrition_df$MaritalStatus,
               "Overtime" = attrition_df$OverTime,
               "Performance Rating" = attrition_df$PerformanceRatingText,
               "Stock Options Level" = attrition_df$StockOptionLevel,
               "Work/Life Balance" = attrition_df$WorkLifeBalanceText)
        })

renderPlot({
        
        fit2 <- survfit(surv_object ~ dimension_choice(), data = attrition_df)
        
        model2_df <- fortify(fit2)
        
        model2_top20_df <- model2_df %>%
                arrange(strata, time) %>%
                mutate(avg_probability_decrease = lag(surv) - surv) %>%
                filter(time < 40 & time > 0)
        
        mean_rr2 <- mean(model2_top20_df$avg_probability_decrease, na.rm = TRUE)
        sd_rr2 <- sd(model2_top20_df$avg_probability_decrease, na.rm = TRUE)
        sd15_top_rr2 <- mean_rr2 + (1.5 * sd_rr2)
        
        model2_top20_df <- model2_top20_df %>%
                mutate(risk_increase_size = ifelse(avg_probability_decrease >= sd15_top_rr, ">= 1.5 standard deviations", "< 1.5 standard deviations")) %>%
                filter(time <= 20 & time > 0)
        
        level_count <- length(model2_df$strata)
        
        survival2_viz <- ggplot(model2_df, aes(x = time, y = surv, color = strata)) +
                geom_line(size = 1.25) +
                scale_color_manual(values = custom_palette[1:level_count]) +
                scale_y_continuous(label = percent_format()) +
                labs(title = "Retention probability analysis", subtitle = "", x = "years at company", y = "retention probability") +
                theme_minimal()
        
        risk2_viz <- ggplot(model2_top20_df, aes(x = reorder(time, desc(time)), y = avg_probability_decrease, fill = risk_increase_size)) +
                geom_bar(stat = "identity") +
                geom_hline(yintercept = mean_rr2, color = "#8F8F8F", size = 2, alpha = 0.5) +
                scale_fill_manual(values = c("#798396", "#ED8800")) +
                scale_y_continuous(label = percent_format()) +
                facet_wrap(~strata) +
                coord_flip() +
                labs(title = "Increase in retention risk for each year of service", subtitle = paste0("(overall average = ", percent(mean_rr2, 3), ")"), x = "tenure (years)", y = "average retention risk increase") +
                theme(panel.background = element_blank(),
                      panel.grid.major.x = element_line(color = "#dee2e8"))
        
        grid.arrange(survival2_viz, risk2_viz, ncol = 2)
        
        })

```

#### Driver Analysis

Understanding the factors that are more highly correlated with attrition can help us investigate root causes and develop programs, policies or processes to address those root causes. Note that **correlation is not causation**. If we find correlations between a factor and attrition, we know that there is a relationship *but we don't know why* there is a relationship. For example, if there is a high correlation between lower Employee Satisfaction Survey scores and attrition, then we know that employees who leave may tend to have lower Employee Satisfaction Survey scores, but we don't know specifically why the employees left. The reasons could vary widely and require further root cause investigation.

We will apply a two-step process to find the drivers of Shadowrun's attrition. We will apply an initial first-pass methodology to determine which factors may be the most important. We will then apply a second-pass methodology to determine which levels within each factor are the most significant drivers of attrition. For example, our first pass may find that the Job Description is an important factor. Knowing that the employee's job is a factor in attrition is important, but it is not helpful unless we know which jobs have the greatest impact on attrition.

The factor importance chart below show us the relative importance of all of the factors in our dataset. This first pass applies a random forest algorithm to compute the relative importance of each factor. Note that you will get different results depending on the algorithm or method you choose.

```{r echo = FALSE, message=FALSE, warning=FALSE}

renderPlot({
        
        variable_viz
        
        })

```
<br>
<br>
Now that we know the relative importance of each factor, the next step is to decide which factors to include in our model. The drop-down below allows you to select the Importance threshold for those factors you want to include in the model. After you choose your desired threshold, we apply the second-pass step of including the selected factors in a logistic regression model. The output table that you will see when you select your threshold includes the output from the model. The coefficient column tells us whether the relationship between that factor level and attrition is positive or negative and the degree to which they are related. Values at or near zero have very little influence on attrition. The p-value column tells us whether the relationship is significant. For purposes of our analysis we will use a significance level of 0.05 (p <= 0.5) but it is important to note that there are [debates in the scientific community](https://www.nature.com/articles/s41562-017-0189-z) swelling around the appropriate significance level to use. 

```{r echo = FALSE, message=FALSE, warning=FALSE}

# Set Gini index cutoff

numericInput(inputId = "gini_cutoff", label = "Choose the Gini cut-off point below which you want to exclude variables:",
            min = min(driver_imp_rf$Overall), max = max(driver_imp_rf$Overall), value = 15)

render_gt({
        
        options(scipen = 999)
        
        variables_chosen <- driver_imp_rf %>% filter(Overall >= input$gini_cutoff) %>% dplyr::select(driver)
        
        #variables_chosen <- driver_imp_rf %>% filter(Overall >= 1) %>% dplyr::select(driver)
        
        variables_chosen <- as.vector(variables_chosen$driver)
        
        driver_data_f_df <- driver_data_df %>% dplyr::select(EmployeeNumber, variables_chosen, terminated)
        
        model_driver_lr <- glm(terminated ~ ., data = driver_data_f_df[, -1], family = "binomial")
        
        model_driver_lr_df <- tidy(model_driver_lr)
        
        model_driver_lr_df <- model_driver_lr_df %>%
                dplyr::select(term, estimate, p.value) %>%
                mutate(usefulness_category = ifelse(p.value > 0.05, "not significant (p-value > 0.05)",
                                                    ifelse(p.value <= 0.05 & abs(estimate) > 0.1, "significant and meaningful (p-value <= 0.05 and coefficient > 0.1 or < -0.1)", 
                                                    ifelse(p.value <= 0.05 & abs(estimate) <= 0.1, "significant but not meaningful (p-value <= 0.05 and coefficient between 0.1 and -0.1)", "unknown"))),
                       usefulness_cat_num = ifelse(p.value > 0.05, -1,
                                                    ifelse(p.value <= 0.05 & abs(estimate) > 0.1, 1, 
                                                    ifelse(p.value <= 0.05 & abs(estimate) <= 0.1, 0, 0)))) %>%
                arrange(desc(usefulness_cat_num), desc(estimate), term) %>%
                mutate(estimate = round(estimate, 3),
                       p.value = round(p.value, 4)) %>%
                dplyr::select(-usefulness_cat_num)
        
        names(model_driver_lr_df) <- c("factor", "coefficient", "p.value", "usefulness_category")
                
        g_table <- model_driver_lr_df %>%
                gt(groupname_col = "usefulness_category") %>%
                tab_options(table.font.size = 10,
                    row.padding = px(1)
                    )
        
        g_table
        
})

```
<br>
<br>

# Retention Risk Scoring
Now that we have analyzed which factors and factor levels are correlated with attrition and the direction and strength of that relationship, let's leverage that data to assist with decision-making and more focused analysis. We will develop a more fine-tuned model using machine learning techniques to predict which of our active employees may be at risk of leaving. The general process we follow to build a machine learning model is:

  1. data collection;
  2. data preparation;
  3. building the model;
  4. training the model;
  5. evaluating model performance; and
  6. improving model performance.

### Data Collection and Preparation

We have already completed a large portion of data collection and preparation for the previous analyses. A data collection and preparation and processing step that is unique to machine learning is to split data into "training" and "test" datasets. The training dataset is used to build the model and the test dataset is used to evaluate the accuracy and precision of the model. The split is usually a percentage split with the larger percentage being put in the training dataset and the smaller percentage put into the test dataset. Common splits are 70%/30% and 80%/20%. We will use an 80/20 split for our analysis.


### Build and Train Models

When building a machine learning model, it can be beneficial to try a few different algorithms and compare their performance. For purposes of this illustration, we will build models using the naive Bayes, logistic regression, decision tree and random forest algorithms. We will not discuss these algorithms in detail. A confusion matrix is an important tool in helping assess the performance of a model. Several metrics can be generated from a confusion matrix. These metrics help us decide whether the model or models will work for our purposes. Following are the confusion matrices for each of the models we created.

```{r echo = FALSE, message=FALSE, warning=FALSE}

confusion_matrix_et_w %>%
        gt() %>%
        tab_spanner(label = md("*Logistic Regression*"), columns = vars(n_lr, pct_lr)) %>%
        tab_spanner(label = md("*Naive Bayes*"), columns = vars(n_nb, pct_nb)) %>%
        tab_spanner(label = md("*Random Forest*"), columns = vars(n_rf, pct_rf)) %>%
        tab_spanner(label = md("*Regression Tree*"), columns = vars(n_rt, pct_rt)) %>%
        cols_label(n_lr = "n", pct_lr = "%",
                   n_nb = "n", pct_nb = "%",
                   n_rf = "n", pct_rf = "%",
                   n_rt = "n", pct_rt = "%") %>%
        fmt_percent(c(5, 7, 9, 11), decimals = 1) %>%
        tab_options(table.font.size = 12,
                    row.padding = px(1)
                    )

```
<br>
<br>
Following are the metrics derived from the confusion matrices. Different metrics will have different levels of importance depending on the business problem at hand. 
<br>

```{r echo = FALSE, message=FALSE, warning=FALSE}

perf_metrics_et_w_df %>%
        gt() %>%
        cols_label(metric_result_lr = "logistic regression",
                   metric_result_nb = "naive Bayes",
                   metric_result_rf = "random forest",
                   metric_result_rt = "regression tree") %>%
        fmt_percent(c(2:5), decimals = 1) %>%
        tab_options(table.font.size = 12,
                    row.padding = px(1)
                    )

```
  

### Evaluate Model Performance
First we built our model using 80% of the data available to us. This initial 80% dataset was our training dataset. We evaluated our initial model performance by applying it to the remaining 20% of our data (test data) and comparing its predicted Attrition with actual Attrition. This training and evaluation approach has some limitations because we are only using 80% of the data available to us to build the model and tested against a relatively small amount of data. We tested a second approach to building the models called cross-validation. This method uses all of the data as both training and test data. A percentage of data is held out as a test set and tested against the remaining data--just as it is in the train/test split approach. The difference is that test set is put back in as part of the training data and another set is withheld as the test set. Each hold-out test set is called a "fold". For our purposes, we will use 10-fold cross-validation, which means that we will use 90% of the data as training data and the remaining 10% as test data and we will repeat this 10 times so that all of the data is used in one of the 10% test data "folds".  


```{r echo = FALSE, message=FALSE, warning=FALSE}

confusion_matrix_combo %>%
        gt(group = "algorithm") %>%
        tab_spanner(label = md("*80/20*"), columns = vars(n_ut, pct_ut)) %>%
        tab_spanner(label = md("*Cross Validation*"), columns = vars(n_tr, pct_tr)) %>%
        cols_label(n_ut = "n", pct_ut = "%", n_tr = "n", pct_tr = "%") %>%
        fmt_percent(c(5, 7), decimals = 1) %>%
        tab_options(table.font.size = 12,
                    row.padding = px(1)
                    )

```
<br>  

Two key insights jump out from comparing the 80/20 to the cross-validated models:  1) Balanced Accuracy dropped significantly for naive Bayes; and 2) random forest perfectly predicted all outcomes. 

The Balanced Accuracy for the naive Bayes model has gone from 76% with the 80/20 approach to 50% with the cross-validation approach. That's because the cross-validation approach predicted that all employees would remain active--as we saw above. Later on in the model performance improvement section we will assign costs to certain error types that will adjust the model to take a more balanced approach.

The cross-validated random forest perfectly predicted the outcomes--shouldn't we just use that model? No. This simply means that the random forest found a way to perfectly learn our data and is over-fitted to the data it learned. The model would likely not perform well on data from which it had not learned. Rather than doing a cross-validation on the entire dataset, we should probably cross-validate on an 80% sample and test the model against the remaining 20%. That would give us a better understanding of how the model would perform on data it hasn't seen before.


```{r echo = FALSE, message=FALSE, warning=FALSE}

perf_metrics_df %>%
        gt() %>%
        tab_spanner(label = md("*Logistic Regression*"), columns = vars(metric_result_lr, metric_result_lr_tr)) %>%
        tab_spanner(label = md("*Naive Bayes*"), columns = vars(metric_result_nb, metric_result_nb_tr)) %>%
        tab_spanner(label = md("*Random Forest*"), columns = vars(metric_result_rf, metric_result_rf_tr)) %>%
        tab_spanner(label = md("*Regression Tree*"), columns = vars(metric_result_rt, metric_result_rt_tr)) %>%
        cols_label(metric_result_lr = "80/20", metric_result_lr_tr = "Cross Validation",
                   metric_result_nb = "80/20", metric_result_nb_tr = "Cross Validation",
                   metric_result_rf = "80/20", metric_result_rf_tr = "Cross Validation",
                   metric_result_rt = "80/20", metric_result_rt_tr = "Cross Validation") %>%
        fmt_percent(c(2:9), decimals = 1) %>%
        tab_options(table.font.size = 12,
                    row.padding = px(1)
                    )

```


### Improve Model Performance
In this section, we will select one model and performance tune it. For an actual project, analysts would be wise to performance tune all reasonably feasible models. This analysis will not go into ensemble modeling, but blending of models would fall within the scope of improving model performance.

So far the best model seems to be the cross-validated logistic regression model because it:  

  1. has the best balance between recall and sensitivity versus the other models;
  2. has the highest balanced accuracy; and
  3. has one of the lowest accuracy p-values.
  
Let's try two more approaches to see if we can get better performance. First, all the models we've built so far used all the features in our dataset. We know that some of the features are not significant and some of the features could have multi-colinearity (they could be measuring the same or similar things). Therefore, let's implement feature selection and assessing "goodness-of-fit" measures to try to determine the optimal number of features to include in our cross-validated logistic regression model. 

Finally, let's try a new model--the C5.0 decision tree. This model will allow us to set cost consequences. These cost assumptions help our model understand whether certain incorrect predictions are more costly than other incorrect predictions. Imagine that we are applying our model to our currently active employee population to find areas of retentionn risk. If we incorrectly predict that an active employee is going to leave when they in fact are not a retention risk, then perhaps we've spent time and resources to retain someone whom we were not going to lose. But the more expensive incorrect prediction is when an employee is a retention risk and we did not predict them as such. The employee will leave the company and we missed the opportunity to retain her or him--not to mention incurred the costs associated with that employee's turnover. We can set up a cost structure for correct and incorrect predictions such that the model will try to optimize predictions for the highest-cost area, which is to correctly identify employees at risk for our project.


####Feature Selection
Let's start with intuitively looking at the data for highly-related variables. Our dataset includes columns for the employee's Hourly Rate, Daily Rate, Monthly Rate and Monthly Income. These four variables all measure the same factor--the employee's income. All of the employees in this dataset have the same Standard Hours, so we can use the same formula to calculate their Annual Wages. We will create a single variable called Annual Wages that multiplies their monthly income by 12, then remove these four variables from our dataset. We will then run a random forest algorithm to calculate the importance of each factor and will use the Wald test to assess statical significance of each factor.

```{r include=TRUE, echo = FALSE, message=FALSE, warning=FALSE}

feature_select_print_df %>%
        gt(group = "significant") %>%
        fmt_percent(4, decimals = 4) %>%
        fmt_number(3, decimals = 1) %>%
        fmt_number(5, decimals = 3) %>%
        tab_options(table.font.size = 12,
                    row.padding = px(1)
                    )

```
<br>
<br>

Seven factors are not significant (p-value > 0.05). That leaves us with 20 factors. That's still a lot of variables to include in our model, but they could all add value. In addition, we may still want to include some of the 7 that are showing as not significant. Let's take a look at a goodness-of-fit measure for the various models we could construct with these 27 factors. We will use a pseudo R-squared measure called McFadden's R to assess the goodness of fit of the models.

The chart below shows the increase in McFadden's R with each factor added to the model. The first attribut with which we start is Age and a model that only uses Age has a McFadden's R of 0.03. This means that the model explains about 3% of the variability in the data. Not a very good model. Adding the Overtime factor to the model increases McFadden's R from 0.03 to 0.97. Simply adding the Overtime model increases the power of the model such that it explains an additional 7% of the variability in the data. The first 20 significant variables add much more to the model than the last 7 not significant variables. The first 20 factors get the model to a 0.356 McFadden's R and the remaining 7 factors only get the McFadden's R to 0.359.
<br>

```{r pseudo_r_calc, include=TRUE, echo=FALSE, message=FALSE, warning=FALSE}

renderPlot({

        ggplot(feature_select_df, aes(x = attribute_count, y = mcfadden_r, color = significant)) +
                geom_line(size = 4) +
                labs(title = "McFadden R increase with addition of attribute to model",
                     subtitle = "(significant and not-significant attributes)", x = "attribute count", y = "") +
                scale_color_manual(values = custom_palette[c(4, 1)]) +
                theme_minimal()

})

```

Given the preceding analysis, we will use the 20 significant variables in our model and drop the remaining 7 non-significant variables. As you can see in the following tables, performance has improved slightly by dropping those 7 non-significant variables. Balanced accuracy has improved by 0.4%--every little bit counts! Given that we gain slightly in performance with a model that is less complex (20 attributes versus 27), it's worth the effort to reduce the features of the model.
<br>

```{r echo = FALSE, message=FALSE, warning=FALSE}

cm_lr_compare %>%
        gt() %>%
        tab_header(title = "Logistic Regression Model Comparison",
                   subtitle = "Confusion Matrices") %>%
        tab_spanner(label = md("*Feature Selected Cross Validation*"), columns = vars(n_fs, pct_fs)) %>%
        tab_spanner(label = md("*80/20*"), columns = vars(n_ut, pct_ut)) %>%
        tab_spanner(label = md("*Cross Validation*"), columns = vars(n_tr, pct_tr)) %>%
        cols_label(n_ut = "n", pct_ut = "%", n_tr = "n", pct_tr = "%", n_fs = "n", pct_fs = "%") %>%
        fmt_percent(c(5, 7, 9), decimals = 1) %>%
        tab_options(table.font.size = 12,
                    row.padding = px(1)
                    )

```
<br>
<br>
```{r echo = FALSE, message=FALSE, warning=FALSE}

perf_metrics_lr %>%
        gt() %>%
        tab_header(title = "Logistic Regression Model Comparison",
                   subtitle = "Performance Metrics") %>%
        cols_label(metric_result_lr = "80/20", metric_result_lr_tr = "Cross Validation", 
                   metric_result_lr_fs = "Feature Selected Cross Validation") %>%
        fmt_percent(c(2:4), decimals = 1) %>%
        tab_options(table.font.size = 12,
                    row.padding = px(1)
                    )

```
<br>

####C5.0 Decision Tree
Now let's see if we can out-perform all the models we've built so far. The C5.0 decision tree model is commonly used because the default settings perform relatively well for most situations. We are going to assign cost consequences to this model. For our predictor model there are two types of errors:  1) an active employee is incorrectly predicted to be a retention risk; and 2) a retention risk employee is predicted to be a non-retention risk. We want to minimize the errors in which true retention-risk employees are classified as non-retention risk employees because we miss the opportunity to intervene before the employee leaves. If we mis-classify an active employee as a retention risk, then our loss is only the time and effort we spent trying to retain someone who did not intend to leave. We will assign greater costs to errors that mis-classify true retention risks. We will make an assumption that incorrect classifications of true retention risks are 8 times more costly than incorrect classifications of true non-retention risks. 

<br>
For this model, we will revert back to using the original 27 attributes since the decision tree model will makes its own decisions about which factors and levels are important and which are not.
<br>

```{r echo=FALSE, message=FALSE, warning=FALSE}

c5_cm_tbl_print %>%
        gt() %>% 
        tab_header(title = "C5.0 Decision Tree",
                   subtitle = "Confusion Matrix") %>%
        tab_spanner(label = md("*cost controlled adaptive boost*"), columns = vars(n_c5, pct_c5)) %>%
        cols_label(n_c5 = "n", pct_c5 = "%") %>%
        fmt_percent(5, decimals = 1) %>%
        tab_options(table.font.size = 12,
                    row.padding = px(1)
                    )

```

<br>

```{r echo=FALSE, message=FALSE, warning=FALSE}

c5_perf_measures_print %>%
        gt() %>%
        tab_header(title = "C5.0 Decision Tree",
                   subtitle = "Performance Metrics") %>%
        cols_label(metric_result_c5 = "Cost Controlled Adaptive Boost") %>%
        fmt_percent(2, decimals = 1) %>%
        tab_options(table.font.size = 12,
                    row.padding = px(1)
                    )

```

<br>

#### Model Comparison
We have created 10 different models:  3 logistic regressions, 2 naive Bayes, 2 random forests, 2 regression trees and 1 C5.0 decision tree. When we put the confusion matrix and performance metrics for each model side-by-side we begin to get a picture of which model or models may perform the best with our data. As mentioned before, the cross-validated random forest correctly predicted 100% of the outcomes, but this model likely overfits the data and would not perform well with new data. The cross-validated naive Bayes model simply predicted all outcomes to be active since that would yield the highest accuracy. This model won't do us any good because we need to be able to predict employees who are at risk of leaving and this model wouldn't do that.

If we throw out the cross-validated random forest and cross-validated naive Bayes models as an anomalies, then the range in percent of correct predictions from the worst to the best model is only a difference of 6.7%. The best model--in terms of percentage of correct predictions--is the feature-selected and cross-validated logistic regression, which correctly predicted 81.6% of the actives and 8% of the terms for a total of 89.6% correct. The worst model is the 80/20 split-trained naive Bayes model, which correctly predicted 72.3% of the actives and 10.6% of the terms for a total of 82.9% correct. The cross-validated logistic regression and C5.0 decision tree models performed well also, with 89.4% and 88.4% total correct respectively.

<br>

```{r echo=FALSE, message=FALSE, warning=FALSE}

confusion_matrices_master_df %>%
        gt(group = "algorithm") %>%
        tab_spanner(label = md("*Logistic Regression*"), columns = vars(pct_lr, pct_lr_cv, pct_fs)) %>%
        tab_spanner(label = md("*Naive Bayes*"), columns = vars(pct_nb, pct_nb_cv)) %>%
        tab_spanner(label = md("*Random Forest*"), columns = vars(pct_rf, pct_rf_cv)) %>%
        tab_spanner(label = md("*Regression Tree*"), columns = vars(pct_rt, pct_rt_cv)) %>%
        tab_spanner(label = md("*C5.0 Decision Tree*"), columns = vars(pct_c5)) %>%
        cols_label(pct_lr = "80/20", pct_lr_cv = "Cross Validation", pct_fs = "Feature Selection & Cross Validation",
                   pct_nb = "80/20", pct_nb_cv = "Cross Validation",
                   pct_rf = "80/20", pct_rf_cv = "Cross Validation",
                   pct_rt = "80/20", pct_rt_cv = "Cross Validation",
                   pct_c5 = "Cost Controlled Adaptive Boosting") %>%
        fmt_percent(c(4:13), decimals = 1) %>%
        tab_options(table.font.size = 12,
                    row.padding = px(1)
                    )

```

<br>

So far we're leaning toward the feature-selected and cross-validated logistic regression model as the model we will choose to operationalize. Before we finalize our decision, let's compare the performance metrics of the models to see if anything points us to a different model.

For purposes of this example, we will look for metrics that balance across all of the metrics. Balanced accuracy and F1 are metrics that help us assess whether our model is balanced.

<br>

```{r echo=FALSE, message=FALSE, warning=FALSE}

perf_metrics_master_df %>%
        gt() %>%
        tab_spanner(label = md("*Logistic Regression*"), columns = vars(metric_result_lr, metric_result_lr_tr, metric_result_lr_fs)) %>%
        tab_spanner(label = md("*Naive Bayes*"), columns = vars(metric_result_nb, metric_result_nb_tr)) %>%
        tab_spanner(label = md("*Random Forest*"), columns = vars(metric_result_rf, metric_result_rf_tr)) %>%
        tab_spanner(label = md("*Regression Tree*"), columns = vars(metric_result_rt, metric_result_rt_tr)) %>%
        tab_spanner(label = md("*C5.0 Decision Tree*"), columns = vars(metric_result_c5)) %>%
        cols_label(metric_result_lr = "80/20", metric_result_lr_tr = "Cross Validation", metric_result_lr_fs = "Feature Selection w/ Cross Validation",
                   metric_result_nb = "80/20", metric_result_nb_tr = "Cross Validation",
                   metric_result_rf = "80/20", metric_result_rf_tr = "Cross Validation",
                   metric_result_rt = "80/20", metric_result_rt_tr = "Cross Validation",
                   metric_result_c5 = "Cost Controlled Adaptive Boost") %>%
        fmt_percent(c(2:11), decimals = 1) %>%
        tab_options(table.font.size = 12,
                    row.padding = px(1)
                    )

```

<br>

#### Receiver Operating Characteristics (ROC) Curve
Receiver Operating Characteristics (ROC) curves are a popular way to visualize the performance of predictive models. You can use the selection boxes below to choose two models you would like to compare. The chart will show the respective ROC curve for each model in addition to its area under the curve (AUC). The AUC metric is a single metric that tells us how well the model predicts. AUC values closer to 1.0 are better performing models and AUC values closer to 0.5 are poorer performing models. The ROC curve shows us the percentage of false positive predictions relative to the percentage of true positives.

```{r echo=FALSE, message=FALSE, warning=FALSE}

selectInput(inputId = "model1_select", label = "Select the first model you'd like to compare:  ",
            choices = c("Logistic Regression - 80/20 Validation", "Naive Bayes - 80/20 Validation",
                        "Random Forest - 80/20 Validation", "Regression Tree - 80/20 Validation",
                        "Logistic Regression - 10-Fold Cross Validation", "Naive Bayes - 10-Fold Cross Validation",
                        "Random Forest - 10-Fold Cross Validation", "Regression Tree - 10-Fold Cross Validation",
                        "Logistic Regression - Feature-Selected 10-Fold Cross Validation", "C5.0 Adaptive Boosted Decision Tree"),
            selected = "Naive Bayes - 80/20 Validation")

selectInput(inputId = "model2_select", label = "Select the second model you'd like to compare:  ",
            choices = c("Logistic Regression - 80/20 Validation", "Naive Bayes - 80/20 Validation",
                        "Random Forest - 80/20 Validation", "Regression Tree - 80/20 Validation",
                        "Logistic Regression - 10-Fold Cross Validation", "Naive Bayes - 10-Fold Cross Validation",
                        "Random Forest - 10-Fold Cross Validation", "Regression Tree - 10-Fold Cross Validation",
                        "Logistic Regression - Feature-Selected 10-Fold Cross Validation", "C5.0 Adaptive Boosted Decision Tree"),
            selected = "Naive Bayes - 10-Fold Cross Validation")

model1_pick <- reactive({
        switch(input$model1_select,
               "Naive Bayes - 80/20 Validation" = pred_nb_80,
               "Logistic Regression - 80/20 Validation" = pred_lr_80,
               "Random Forest - 80/20 Validation" = pred_rf_80,
               "Regression Tree - 80/20 Validation" = pred_rt_80,
               "Naive Bayes - 10-Fold Cross Validation" = pred_nb_cv,
               "Logistic Regression - 10-Fold Cross Validation" = pred_lr_cv,
               "Random Forest - 10-Fold Cross Validation" = pred_rf_cv,
               "Regression Tree - 10-Fold Cross Validation" = pred_rt_cv,
               "Logistic Regression - Feature-Selected 10-Fold Cross Validation" = pred_lr_fs_cv,
               "C5.0 Adaptive Boosted Decision Tree" = pred_c5_ab)
        })

model2_pick <- reactive({
        switch(input$model2_select,
               "Naive Bayes - 80/20 Validation" = pred_nb_80,
               "Logistic Regression - 80/20 Validation" = pred_lr_80,
               "Random Forest - 80/20 Validation" = pred_rf_80,
               "Regression Tree - 80/20 Validation" = pred_rt_80,
               "Naive Bayes - 10-Fold Cross Validation" = pred_nb_cv,
               "Logistic Regression - 10-Fold Cross Validation" = pred_lr_cv,
               "Random Forest - 10-Fold Cross Validation" = pred_rf_cv,
               "Regression Tree - 10-Fold Cross Validation" = pred_rt_cv,
               "Logistic Regression - Feature-Selected 10-Fold Cross Validation" = pred_lr_fs_cv,
               "C5.0 Adaptive Boosted Decision Tree" = pred_c5_ab)
        })

renderPlot({
        
        model1 <- input$model1_select
        model2 <- input$model2_select
        
        pred1 <- model1_pick()
        pred2 <- model2_pick()
        
        rocr_perf1 <- performance(pred1, measure = "tpr", x.measure = "fpr")
        
        roc1_df <- data.frame(
                
                model = model1,
                false_positive_rate = unlist(rocr_perf1@x.values),
                true_positive_rate = unlist(rocr_perf1@y.values)
                
                )
        
        auc_roc1 <- roc.curve(roc1_df$true_positive_rate, roc1_df$false_positive_rate)
        
        rocr_perf2 <- performance(pred2, measure = "tpr", x.measure = "fpr")
        
        roc2_df <- data.frame(
                
                model = model2,
                false_positive_rate = unlist(rocr_perf2@x.values),
                true_positive_rate = unlist(rocr_perf2@y.values)
                
                )
        
        auc_roc2 <- roc.curve(roc2_df$true_positive_rate, roc2_df$false_positive_rate)
        
        roc1_df$auc <- round(auc_roc1$auc, 3)
        
        roc1_df <- roc1_df %>%
                mutate(model_label = paste0(model, " (AUC = ", auc, ")"))
        
        roc2_df$auc <- round(auc_roc2$auc, 3)

        roc2_df <- roc2_df %>%
                mutate(model_label = paste0(model, " (AUC = ", auc, ")"))
        
        roc_df <- bind_rows(roc1_df, roc2_df)
        
        roc_viz <- ggplot(roc_df, aes(x = false_positive_rate, y = true_positive_rate, color = model_label)) +
                geom_line(size = 2) +
                scale_color_manual(values = c("#ED8800", "#000000")) +
                geom_abline(intercept = 0, slope = 1, lty = 2, color = "#000000") +
                labs(title = paste0("Performance comparison of ", model1, " and\n", model2),
                     x = "false positive rate", y = "true positive rate") +
                scale_x_continuous(limits = c(0, 1)) +
                scale_y_continuous(limits = c(0, 1)) +
                theme_minimal() +
                theme(legend.position = c(0.65, 0.25),
                      legend.title = element_blank())
        
        roc_viz
        
}, height = 600, width = 550)


```
<br>
<br>

# Conclusion
This analysis is intended to be illustrative of the interactive capabilities of R Markdown. I used an attrition analysis example based on a simple, publicly available 1,400 row by 35 column spreadsheet of data. I leveraged several analysis and modeling techniques to show how interactive R Markdown can be helpful for both exploratory and explanatory analyses. This analysis is not guaranteed to be error-free and is not an exhaustive or complete analysis. I hope you've found value in this analysis and that it has given you ideas that you'd like to apply to your own work and projects. I'd be very interested to hear whether this example has helped you--feel free to e-mail me at <jidle1975@gmail.com>. Check out the references at the end of this analysis--I've listed some excellent books and online articles that you may want to explore further. Best wishes in your interactive R Markdown endeavors!

<br>
<br>

# References and Resources
Following are the online articles and resources I used in compiling this analysis:

  * **Employee attrition articles and references >>**
    * *Extended Tutorial: How to Predict Employee Turnover >>* https://www.hranalytics101.com/extended-tutorial-how-to-predict-employee-turnover/
  * **Sample attrition data posted by IBM >>** https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/
  * **Interactive Shiny documents >>**
    * https://bookdown.org/yihui/rmarkdown/shiny-documents.html
    * https://rmarkdown.rstudio.com/lesson-14.html
  * **Data visualization >>**
    * *Data visualization: A Practical Introduction*, (2018) by Kieran Healy, https://socviz.co/index.html#preface
  * **Survival analysis >>** https://www.datacamp.com/community/tutorials/survival-analysis-R
  * **Feature selection >>** 
    * https://www.r-bloggers.com/feature-selection-using-the-caret-package/
    * http://dataaspirant.com/2018/01/15/feature-selection-techniques-r/
    * https://www.r-bloggers.com/variable-importance-plot-and-variable-selection/
    * https://www.datacamp.com/community/tutorials/feature-selection-R-boruta
    * https://freakonometrics.hypotheses.org/19835
    * https://blog.datadive.net/selecting-good-features-part-iii-random-forests/
  * **Machine Learning/Predictive Modeling >>**
    * *Machine Learning with R*, (2013) by Brett Lantz
    * *Data Science for Business*, (2013) by Foster Provost & Tom Fawcett
    * https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7
    * **Naive Bayes >> ** 
      * https://www.r-bloggers.com/understanding-naive-bayes-classifier-using-r/
      * *Naive Bayes Classifier Essentials >>* http://www.sthda.com/english/articles/36-classification-methods-essentials/145-naive-bayes-classifier-essentials/
    * **Logistic regression >>** 
      * https://www.datacamp.com/community/tutorials/logistic-regression-R
      * https://www.r-bloggers.com/evaluating-logistic-regression-models/
    * **Performance tuning >>** http://www.socr.umich.edu/people/dinov/courses/DSPA_notes/14_ImprovingModelPerformance.html 
  * **Online tools >>** 
    * https://www.google.com/search?q=color+picker
  * **Technical >>** https://stackoverflow.com/
    * https://stackoverflow.com/questions/8499361/easy-way-of-counting-precision-recall-and-f1-score-in-r
    * **gtable >>** 
      * https://github.com/rstudio/gt/blob/master/README.md
      * https://www.rdocumentation.org/packages/gt/versions/0.1.0
    * **caret >>**
      * https://cran.r-project.org/web/packages/caret/vignettes/caret.html
      * http://www.rebeccabarter.com/blog/2017-11-17-caret_tutorial/




