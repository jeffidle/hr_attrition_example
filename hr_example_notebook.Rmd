---
title: "Example HR research using interactive R Markdown"
date: "February 4, 2019"
author: "Jeff Idle"
output: html_notebook
runtime: shiny
editor_options: 
  chunk_output_type: inline
---

# Purpose and Background
The purpose of this analysis is to showcase the capabilities of interactive R Markdown documents. R Markdown documents are great for integrating analysis with visualizations. Interactive R Markdown documents provide a better way to engage your stakeholders and let them explore your analysis further. IBM posted a sample Human Resources Employee Attrition and Performance dataset on their [Watson Analytics Community web site](https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/). The imaginary stakeholders for this example analysis will be the CEO and HR VP for a fictional medium-sized company. Our fictional company researches and designs productivity and entertainment peripherals that can be biologically integrated with the human body and connect to the IoT (Internet of Things). Our fictional company is called "Shadowrun, Inc.". The dataset does not contain hire dates or termination dates, so for purposes of this analysis we will assume all of the attrition in our dataset represents voluntary turnover for the last five years. The source code for this analysis can be found on Github at https://github.com/jeffidle/hr_attrition_example.

**NOTE:**  Please note that this analysis is an example for illustrative purposes only. As such, this analysis is not guaranteed to be error-free or copied without further analysis and modifications to fit the situation to which it is being applied. 


# Business Questions
Our analytics team met with Shadow Run's CEO and HR VP to discuss their critical workforce planning and management questions. They identified and prioritized research into the following three questions:

  1. At what levels of service with the company do we have the greatest risk of losing employees?
  2. What are the key drivers of Shadow Run's turnover?
  3. Can we develop a model to predict the risk of losing our currently active employees?


# Executive Summary
Our analytics team found the following:

  * **High risk levels of service >>** TBD
  * **Voluntary turnover drivers >>** TBD
  * **Turnover risk model >>** TBD


# Methodologies
We utilized several methodologies in answering the business questions. Following are the methods we applied for each business question.

  * **Retention risk by years of service >>** Survival analysis
  * **Voluntary turnover drivers >>** Random forest & logistic regression
  * **Retention risk model >>** Machine learning, logistic regression, naive Bayes, random forest & regression trees


# Analysis
The analysis that follows will start with exploratory analyses to understand the data. We will then analyze the data to answer the question about years of service where retention risk is greatest. Next we will analyze the key drivers that are highly correlated with turnover during the last 5 years. Finally, we will create a predictive model that enables us to assess the risk of losing our currently active employees and estimate the risk of losing any new employees that join the organization.

```{r echo = FALSE, message=FALSE, warning=FALSE}

library(tidyverse)
library(shiny)
library(survival)
library(survminer)
library(ggfortify)
library(scales)
library(lubridate)
library(caret)
library(randomForest)
library(ipred)
library(gbm)
library(broom)
library(rlang)
library(gridExtra)
library(klaR)
library(rpart)
library(gt)

```

```{r echo = FALSE, message=FALSE, warning=FALSE}

attrition_df <- read.csv("data/ibm_attrition.csv", stringsAsFactors = FALSE)
attrition_ref_data_df <- read.csv("data/ibm_attrition_ref_data.csv", stringsAsFactors = FALSE)
#training_df <- read.csv("data/ibm_training.csv", stringsAsFactors = FALSE)

```

```{r echo = FALSE, message=FALSE, warning=FALSE}

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "Education", 2:3], by.x = "Education", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "EducationText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "EnvironmentSatisfaction", 2:3], by.x = "EnvironmentSatisfaction", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "EnvironmentSatisfactionText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "JobInvolvement", 2:3], by.x = "JobInvolvement", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "JobInvolvementText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "JobSatisfaction", 2:3], by.x = "JobSatisfaction", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "JobSatisfactionText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "PerformanceRating", 2:3], by.x = "PerformanceRating", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "PerformanceRatingText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "RelationshipSatisfaction", 2:3], by.x = "RelationshipSatisfaction", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "RelationshipSatisfactionText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "WorkLifeBalance", 2:3], by.x = "WorkLifeBalance", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "WorkLifeBalanceText"

attrition_df <- attrition_df %>%
        mutate(terminated = ifelse(Attrition == "Yes", 1, 0))

attrition_df <- as.data.frame(unclass(attrition_df))

attrition_data_df <- attrition_df[ , c(16:42, 8:14, 43)]
attrition_data_df$terminated <- factor(attrition_data_df$terminated)
attrition_df$Attrition <- factor(attrition_df$Attrition, levels = c("Yes", "No"))

```

```{r echo = FALSE, message=FALSE, warning=FALSE}

options(scipen = 999)

data_summary_df <- data.frame(
        
        data_element = names(attrition_df),
        data_type = unlist(lapply(attrition_df, class))
        
)

numeric_cols_df <- attrition_df %>% select_if(is.numeric)
integer_cols_df <- attrition_df %>% select_if(is.integer)
num_int_cols_df <- bind_cols(numeric_cols_df, integer_cols_df)
factor_cols_df <- attrition_df %>% select_if(is.factor)
character_cols_df <- attrition_df %>% select_if(is.character)

data_levels <- lapply(factor_cols_df, levels)

min_vals <- lapply(num_int_cols_df, min)
mean_vals <- lapply(num_int_cols_df, mean)
max_vals <- lapply(num_int_cols_df, max)
col_count <- length(data_levels)
col_count2 <- length(min_vals)

for(i in 1:col_count){
        
        tmp_level_df <- data.frame(
                
                data_element = names(data_levels[i]),
                data_level_count = paste0(length(data_levels[[i]]), collapse = ""),
                data_level_text = paste0(data_levels[[i]], collapse = ", ")
                
        )
        
        if(!exists("level_df")){
                
               level_df <- tmp_level_df 
                
        } else level_df <- bind_rows(level_df, tmp_level_df)
        
}

level_df <- level_df %>% mutate(data_summary = paste0("level count = ", data_level_count, "; levels = ", data_level_text))

for(i in 1:col_count2){
        
        tmp_numsum_df <- data.frame(
                
                data_element = names(min_vals[i]),
                min_value = min_vals[[i]],
                mean_value = mean_vals[[i]],
                max_value = max_vals[[i]]
                
        )
        
        if(!exists("numsum_df")){
                
               numsum_df <- tmp_numsum_df 
                
        } else numsum_df <- bind_rows(numsum_df, tmp_numsum_df)
        
}

numsum_df <- numsum_df %>% mutate(data_summary = paste0("min = ", min_value, "; mean = ", round(mean_value, 3), "; max = ", max_value))

col_summaries_df <- bind_rows(level_df[ , c(1, 4)], numsum_df[ , c(1, 5)])

data_summary_df <- merge(data_summary_df, col_summaries_df, by = "data_element", all.x = TRUE, all.y = FALSE)

data_summary_df <- data_summary_df %>%
        arrange(data_type, data_element)

#rm(tmp_numsum_df, numsum_df, tmp_level_df, level_df, data_levels, data_summary_df)

```




#### Exploratory analysis
The starting point for every analysis is to become familiar with the data. You can view the raw data [here](https://github.com/jeffidle/hr_attrition_example/blob/master/data/ibm_attrition.csv) but let's take a look at a summarized view of the data. 

```{r echo = FALSE, message=FALSE, warning=FALSE}

data_summary_df %>% gt() %>%
        tab_options(table.font.size = 10,
                    row.padding = px(1)
                    )

```




Now, let's visualize the data. We would first like to see how are data is distributed among active and terminated records. The following chart shows that 16% of the records in our dataset are voluntary terminations and the remaining 84% are currently active employees.

```{r echo = FALSE, message=FALSE, warning=FALSE}

term_dist_viz <- ggplot(attrition_df, aes(x = Attrition, fill = Attrition)) +
        geom_bar() +
        geom_text(stat = "count", aes(label = ..count..), hjust = -0.5) +
        scale_fill_manual(values = c("#ED8800", "#5378b5")) +
        labs(title = "Employee count by attrition category", subtitle = "(no = currently active; yes = terminated)", x = "", y = "") +
        coord_flip() +
        theme(panel.background = element_blank(),
              axis.line.x = element_blank(),
              axis.line.y = element_blank(),
              axis.ticks.x = element_blank(),
              axis.ticks.y = element_blank(),
              axis.text.x = element_blank(),
              legend.position = "none")

renderPlot({
  term_dist_viz
})

```


The drop-down box below allows you to view the counts of employees by the selected dimension and the distribution of active versus terminated employee records within the levels of that dimension. Feel free to use the drop-down to explore the data further.

```{r echo = FALSE}

selectInput(inputId = "explore_dim", label = "Choose the variable that you would like to explore:",
            choices = c("Gender", "Department", "Education", "Education Field", "Business Travel", "Overtime", "Stock Options Level",
                        "Job Satisfaction", "Work/Life Balance"), selected = "Gender")

dim1_select <- reactive({
        switch(input$explore_dim,
               "Gender" = attrition_df$Gender,
               "Department" = attrition_df$Department,
               "Education" = attrition_df$EducationText,
               "Education Field" = attrition_df$EducationField,
               "Business Travel" = attrition_df$BusinessTravel,
               "Overtime" = attrition_df$OverTime,
               "Stock Options Level" = attrition_df$StockOptionLevel,
               "Job Satisfaction" = attrition_df$JobSatisfactionText,
               "Work/Life Balance" = attrition_df$WorkLifeBalanceText)
        })

renderPlot({
        
        explore_dist_viz <- ggplot(attrition_df, aes(x = dim1_select())) +
                geom_bar(fill = "#8f8f8f") +
                geom_text(stat = "count", aes(label = ..count..), hjust = 1.5, color = "#ffffff") +
                labs(title = paste0("Employee count distribution"), x = "", y = "") +
                coord_flip() +
                theme(panel.background = element_blank(),
                      axis.line.x = element_blank(),
                      axis.line.y = element_blank(),
                      axis.ticks.x = element_blank(),
                      axis.ticks.y = element_blank(),
                      axis.text.x = element_blank(),
                      legend.position = "none")
        
        explore2_dist_viz <- ggplot(attrition_df, aes(x = dim1_select(), y = EmployeeCount, fill = Attrition)) +
                geom_bar(position = "fill", stat = "identity") +
                labs(title = paste0("Active versus terminated distribution"), x = "", y = "") +
                scale_fill_manual(values = c("#ED8800", "#5378b5")) +
                scale_y_continuous(label = percent_format()) +
                coord_flip() +
                theme(panel.background = element_blank(),
                      axis.line.y = element_blank(),
                      axis.ticks.y = element_blank())
        
        explore2_dist_viz
        
        grid.arrange(explore_dist_viz, explore2_dist_viz, ncol = 2)
        
})

```


#### Retention Risk by Years of Service
Understanding whether there are years throughout an employee's tenure in which retention risk is highest can help Shadowrun create focused and targeted retention programs. Targeted programs should be more effective than one-size-fits-all programs because targeted programs are tailored to the specific needs of the population at risk. By leveraging the survival analysis methodology in the context of employee attrition, we can assess the average drop in retention probability for each year of service to identify those years in which retention risk is greatest.

The chart on the left shows how the probability of retaining Shadowrun employees drops as tenure increases. This is expected to some extent--everyone will retire someday and that is part of voluntary termination. If all Shadowrun employees were hired directly from high school or college and stayed until they retired, then the "survival curve" would bow outward to the right. If employees were leaving within relatively short periods of time, then the "survival curve" would bow inward to the left. Shadowrun's curve bows outward to the right, which tells us that our employees tend to stay with us. In fact, our employees have a 25% probability of staying with us for 15 years and 40% probability of staying with us for 31 years. The first half of the curve is generally smooth, which tells us there isn't a lot of year-over-year variation. The jagged portions at the end of the curve exist simply because there is a very small population of employees at those tenure levels. 

The chart on the right shows the average reduction in retention probability for each year of service. This chart focuses on the first 20 years since that constitutes the bulk of our workforce. The overall average decrease in retention probability for each year of service is 1.4% if we exclude the outlier in year 40. We can see in years 1 and 10 that the decrease is around 4%--nearly 3x the average. We should work with Shadowrun's HR Business Partners to better understand what might be going on in these years. The first year spike is not unexpected and may be due to onboarding, poor culutural fit, or other factors common to employees accepting a new job at a new company. The tenth-year spike, however, is unusual and should be investigated further.

```{r echo = FALSE}

surv_object <- Surv(time = attrition_df$YearsAtCompany, event = attrition_df$terminated)

fit1 <- survfit(surv_object ~ 1, data = attrition_df)

model_df <- fortify(fit1)

model_top20_df <- model_df %>%
        arrange(time) %>%
        mutate(avg_probability_decrease = lag(surv) - surv) %>%
        filter(time < 40 & time > 0)

mean_rr <- mean(model_top20_df$avg_probability_decrease, na.rm = TRUE)
sd_rr <- sd(model_top20_df$avg_probability_decrease, na.rm = TRUE)
sd15_top_rr <- mean_rr + (1.5 * sd_rr)

model_top20_df <- model_top20_df %>%
        mutate(risk_increase_size = ifelse(avg_probability_decrease >= sd15_top_rr, ">= 1.5 standard deviations", "< 1.5 standard deviations")) %>%
        filter(time <= 20 & time > 0)

survival_viz <- ggplot(model_df, aes(x = time, y = surv)) +
        geom_line(size = 1.25) +
        scale_y_continuous(label = percent_format()) +
        labs(title = "Retention probability analysis", subtitle = "", x = "years at company", y = "retention probability") +
        theme_minimal()

risk_viz <- ggplot(model_top20_df, aes(x = reorder(time, desc(time)), y = avg_probability_decrease, fill = risk_increase_size)) +
        geom_bar(stat = "identity") +
        scale_fill_manual(values = c("#798396", "#ED8800")) +
        scale_y_continuous(label = percent_format()) +
        geom_hline(yintercept = mean_rr, color = "#8F8F8F", size = 2, alpha = 0.5) +
        annotate("text", label = "- overall average = 1.4%", x = 5, y = 0.016, vjust = 0, hjust = 0.005) +
        coord_flip() +
        labs(title = "Increase in retention risk for each year of service", subtitle = "", x = "tenure (years)", y = "average retention risk increase") +
        theme_minimal()

renderPlot({
  grid.arrange(survival_viz, risk_viz, ncol = 2)
})

```



Overall, we don't have major concerns about retention risk by tenure, but we should investigate the spikes in years 1 and 10. We would be surprised if there is not a policy, program or issue that explains why more employees leave in their 10th year than normal. What we don't know, however, is whether there are segments of the population where there are more clear patterns in higher retention risk by tenure. Use the drop-down below to explore various dimensions. If you slice by Education Field you will notice the employees whose education is in Human Resources have a high retention risk increases in years 2 - 4.

```{r echo = FALSE}

selectInput(inputId = "survival_slice", label = "Choose the dimension by which you'd like to slice the retention analysis:", 
  choices = c("Gender", "Department", "Education", "Education Field", "Business Travel", "Overtime", "Stock Options Level",
              "Job Satisfaction", "Work/Life Balance"), selected = "Gender")

dimension_choice <- reactive({
        switch(input$survival_slice,
               "Gender" = attrition_df$Gender,
               "Department" = attrition_df$Department,
               "Education" = attrition_df$EducationText,
               "Education Field" = attrition_df$EducationField,
               "Business Travel" = attrition_df$BusinessTravel,
               "Overtime" = attrition_df$OverTime,
               "Stock Options Level" = attrition_df$StockOptionLevel,
               "Job Satisfaction" = attrition_df$JobSatisfactionText,
               "Work/Life Balance" = attrition_df$WorkLifeBalanceText)
        })

renderPlot({
        
        fit2 <- survfit(surv_object ~ dimension_choice(), data = attrition_df)
        
        model2_df <- fortify(fit2)
        
        model2_top20_df <- model2_df %>%
                arrange(strata, time) %>%
                mutate(avg_probability_decrease = lag(surv) - surv) %>%
                filter(time < 40 & time > 0)
        
        mean_rr2 <- mean(model2_top20_df$avg_probability_decrease, na.rm = TRUE)
        sd_rr2 <- sd(model2_top20_df$avg_probability_decrease, na.rm = TRUE)
        sd15_top_rr2 <- mean_rr2 + (1.5 * sd_rr2)
        
        model2_top20_df <- model2_top20_df %>%
                mutate(risk_increase_size = ifelse(avg_probability_decrease >= sd15_top_rr, ">= 1.5 standard deviations", "< 1.5 standard deviations")) %>%
                filter(time <= 20 & time > 0)
        
        survival2_viz <- ggplot(model2_df, aes(x = time, y = surv, color = strata)) +
                geom_line(size = 1.25) +
                scale_y_continuous(label = percent_format()) +
                labs(title = "Retention probability analysis", subtitle = "", x = "years at company", y = "retention probability") +
                theme_minimal()
        
        risk2_viz <- ggplot(model2_top20_df, aes(x = reorder(time, desc(time)), y = avg_probability_decrease, fill = risk_increase_size)) +
                geom_bar(stat = "identity") +
                geom_hline(yintercept = mean_rr2, color = "#8F8F8F", size = 2, alpha = 0.5) +
                scale_fill_manual(values = c("#798396", "#ED8800")) +
                scale_y_continuous(label = percent_format()) +
                facet_wrap(~strata) +
                coord_flip() +
                labs(title = "Increase in retention risk for each year of service", subtitle = paste0("(overall average = ", percent(mean_rr2, 3), ")"), x = "tenure (years)", y = "average retention risk increase") +
                theme(panel.background = element_blank(),
                      panel.grid.major.x = element_line(color = "#dee2e8"))
        
        grid.arrange(survival2_viz, risk2_viz, ncol = 2)
        
        })

```

#### Driver Analysis

Important variables

```{r echo = FALSE, message=FALSE, warning=FALSE}

driver_data_df <- attrition_data_df

# Remove StandardHours and Over18 since there is no variation and remove Attrition since it is redundant
driver_data_df <- driver_data_df[ , -c(10, 13, 29)]

colnames(driver_data_df)[32] <- "terminated"

model_driver_rf <- randomForest(terminated ~ ., data = driver_data_df[, -1])
driver_imp_rf <- varImp(model_driver_rf)
driver_imp_rf$driver <- row.names(driver_imp_rf)
driver_imp_rf <- driver_imp_rf[ , c(2, 1)] %>%
        arrange(desc(Overall))

renderPlot({
        
        variable_viz <- ggplot(driver_imp_rf, aes(x=reorder(driver, Overall), y = Overall)) +
                geom_point() +
                geom_segment(aes(x=driver,xend=driver,y=0,yend=Overall)) +
                labs(title = "Variable importance", subtitle = "(generated by random forest model)", x = "", y = "mean decrease in Gini") +
                theme(panel.background = element_blank(),
                      panel.grid.major.x = element_line(color = "#dee2e8")) +
                coord_flip()
        
        variable_viz
        
        })

```

Key drivers

```{r echo = FALSE, message=FALSE, warning=FALSE}

# Set Gini index cutoff

numericInput(inputId = "gini_cutoff", label = "Choose the Gini cut-off point below which you want to exclude variables:",
            min = min(driver_imp_rf$Overall), max = max(driver_imp_rf$Overall), value = 15)

renderTable({
        
        options(scipen = 999)
        
        variables_chosen <- driver_imp_rf %>% filter(Overall >= input$gini_cutoff) %>% dplyr::select(driver)
        
        variables_chosen <- as.vector(variables_chosen$driver)
        
        driver_data_f_df <- driver_data_df %>% dplyr::select(EmployeeNumber, variables_chosen, terminated)
        
        model_driver_lr <- glm(terminated ~ ., data = driver_data_f_df[, -1], family = "binomial")
        
        model_driver_lr_df <- tidy(model_driver_lr)
        
        model_driver_lr_df %>% gt()
        
})


```



# Machine Learning and Predictive Modeling

### Data Collection

```{r echo = FALSE, message=FALSE, warning=FALSE}

modeling_data_df <- attrition_data_df

# Remove StandardHours and Over18 since there is no variation and remove Attrition since it is redundant
modeling_data_df <- modeling_data_df[ , -c(10, 13, 29)]

colnames(modeling_data_df)[32] <- "terminated"

```


### Data Preparation

Split the data into test and training datasets. The training dataset is used to build the model and the test dataset is used to evaluate the accuracy and precision of the model.

```{r echo = FALSE, message=FALSE, warning=FALSE}

set.seed(1)

# Split the data into training and test set
training_samples_df <- modeling_data_df$EmployeeNumber %>% 
  createDataPartition(p = 0.8, list = FALSE)

train_data_df  <- modeling_data_df[training_samples_df, ]
test_data_df <- modeling_data_df[-training_samples_df, ]

```



### Build and Train the Models - Naive Bayes, Decision Trees and Logistic Regression

Naive Bayes classifier

```{r echo = FALSE, message=FALSE, warning=FALSE}

set.seed(1)

# Fit the models
model_nb <- NaiveBayes(terminated ~ ., data = train_data_df[, -1])
model_lr <- glm(terminated ~ ., data = train_data_df[, -1], family = "binomial")
model_rf <- randomForest(terminated ~ ., data = train_data_df[, -1])
model_rt <- rpart(terminated ~ ., data = train_data_df[, -1])

# Make predictions
predictions_nb <- model_nb %>% predict(test_data_df)
predictions_lr_resp <- model_lr %>% predict(test_data_df, type = "response")
predictions_lr <- factor(ifelse(predictions_lr_resp > 0.5, 1, 0), levels = c(0, 1))
predictions_rf <- model_rf %>% predict(test_data_df)
predictions_rt <- model_rt %>% predict(test_data_df, type = "class")

# Model accuracies
model_build_accuracy_nb <- mean(predictions_nb$class == test_data_df$terminated)
model_build_accuracy_lr <- mean(predictions_lr == test_data_df$terminated)
model_build_accuracy_rf <- mean(predictions_rf == test_data_df$terminated)
model_build_accuracy_rt <- mean(predictions_rt == test_data_df$terminated)

# Confusion matrices
model_build_cm_nb <- confusionMatrix(predictions_nb$class, test_data_df$terminated)
model_build_cm_lr <- confusionMatrix(predictions_lr, test_data_df$terminated)
model_build_cm_rf <- confusionMatrix(predictions_rf, test_data_df$terminated)
model_build_cm_rt <- confusionMatrix(predictions_rt, test_data_df$terminated)

# Model performance metrics
# F-Score: (2 * Precision * Recall)/(Precision + Recall) harmonic mean of precision and recall
perf_metrics_nb_untrained <- as.data.frame(c(model_build_cm_nb$overall, model_build_cm_nb$byClass))
perf_metrics_lr_untrained <- as.data.frame(c(model_build_cm_lr$overall, model_build_cm_lr$byClass))
perf_metrics_rf_untrained <- as.data.frame(c(model_build_cm_rf$overall, model_build_cm_rf$byClass))
perf_metrics_rt_untrained <- as.data.frame(c(model_build_cm_rt$overall, model_build_cm_rt$byClass))

metric_labels <- row.names(perf_metrics_nb_untrained)

perf_metrics_untrained_df <- bind_cols(perf_metrics_nb_untrained, perf_metrics_lr_untrained, perf_metrics_rf_untrained,
                                       perf_metrics_rt_untrained)
names(perf_metrics_untrained_df) <- c("naive_bayes_untrained", "logistic_regression_untrained",
                                      "random_forest_untrained", "regression_tree_untrained")
perf_metrics_untrained_df$metric <- metric_labels

perf_metrics_untrained_df <- perf_metrics_untrained_df[ , c(5, 2, 1, 3:4)]

perf_metrics_untrained_df %>%
        gt() %>%
        fmt_percent(c(2:5), decimals = 1)

```



### Evaluate Model Performance



```{r echo = FALSE, message=FALSE, warning=FALSE}

set.seed(1)

# Build the models
model_test_nb_df <- train(terminated ~ ., data = train_data_df[, -1], method = "nb",
                          trControl = trainControl("cv", number = 10))
model_test_lr_df <- train(terminated ~ ., data = train_data_df[, -1], method = "glm", family = "binomial",
                          trControl = trainControl("cv", number = 10))
model_test_rf_df <- train(terminated ~ ., data = train_data_df[, -1], method = "rf",
                          trControl = trainControl("cv", number = 10))
model_test_rt_df <- train(terminated ~ ., data = train_data_df[, -1], method = "rpart",
                          trControl = trainControl("cv", number = 10))

# Make predictions
predicted_classes_nb <- model_test_nb_df %>% predict(test_data_df)
predicted_classes_lr <- model_test_lr_df %>% predict(test_data_df)
predicted_classes_rf <- model_test_rf_df %>% predict(test_data_df)
predicted_classes_rt <- model_test_rt_df %>% predict(test_data_df)

# Calculate the confusion matrices
model_trained_cm_nb <- confusionMatrix(predicted_classes_nb, test_data_df$terminated)
model_trained_cm_lr <- confusionMatrix(predicted_classes_lr, test_data_df$terminated)
model_trained_cm_rf <- confusionMatrix(predicted_classes_rf, test_data_df$terminated)
model_trained_cm_rt <- confusionMatrix(predicted_classes_rt, test_data_df$terminated)

# Model validation metrics
# F-Score: (2 * Precision * Recall)/(Precision + Recall) harmonic mean of precision and recall
perf_metrics_nb_trained <- as.data.frame(c(model_trained_cm_nb$overall, model_trained_cm_nb$byClass))
perf_metrics_lr_trained <- as.data.frame(c(model_trained_cm_lr$overall, model_trained_cm_lr$byClass))
perf_metrics_rf_trained <- as.data.frame(c(model_trained_cm_rf$overall, model_trained_cm_rf$byClass))
perf_metrics_rt_trained <- as.data.frame(c(model_trained_cm_rt$overall, model_trained_cm_rt$byClass))

metric_labels <- row.names(perf_metrics_nb_trained)

perf_metrics_trained_df <- bind_cols(perf_metrics_nb_trained, perf_metrics_lr_trained, perf_metrics_rf_trained, perf_metrics_rt_trained)
names(perf_metrics_trained_df) <- c("naive_bayes_trained", "logistic_regression_trained", "random_forest_trained", "regression_tree_trained")
perf_metrics_trained_df$metric <- metric_labels

perf_metrics_trained_df <- perf_metrics_trained_df[ , c(5, 2, 1, 3:4)]

perf_metrics_trained_df %>%
        gt() %>%
        fmt_percent(c(2:5), decimals = 1)

```


### Improve Model Performance


```{r echo = FALSE, message=FALSE, warning=FALSE}

set.seed(1)

options(scipen = 999)

feature_select_lr <- varImp(model_test_lr_df)$importance

```



```{r echo = FALSE, message=FALSE, warning=FALSE}

set.seed(1)

options(scipen = 999)

varImp(model_test_lr_df)
varImp(model_test_nb_df)
varImp(model_test_rf_df)
varImp(model_test_rt_df)

model_input_data_df <- attrition_data_df[ , c(2:9, 11:28, 30:35)]

fit_rf <- randomForest(terminated ~ ., data = model_input_data_df)

# Create an importance based on mean decreasing gini
gini_df <- as.data.frame(importance(fit_rf))

```



# Conclusion


# References and Resources
Following are the online articles and resources I used in compiling this analysis:

  * **Employee attrition articles and references >>**
    * *Extended Tutorial: How to Predict Employee Turnover >>* https://www.hranalytics101.com/extended-tutorial-how-to-predict-employee-turnover/
  * **Sample attrition data posted by IBM >>** https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/
  * **Interactive Shiny documents >>**
    * https://bookdown.org/yihui/rmarkdown/shiny-documents.html
    * https://rmarkdown.rstudio.com/lesson-14.html
  * **Survival analysis >>** https://www.datacamp.com/community/tutorials/survival-analysis-R
  * **Feature selection >>** 
    * https://www.r-bloggers.com/feature-selection-using-the-caret-package/
    * http://dataaspirant.com/2018/01/15/feature-selection-techniques-r/
    * https://www.r-bloggers.com/variable-importance-plot-and-variable-selection/
  * **Machine Learning/Predictive Modeling >>** *Machine Learning with R*, (2013) by Brett Lantz 
    * **Naive Bayes >> ** 
      * https://www.r-bloggers.com/understanding-naive-bayes-classifier-using-r/
      * *Naive Bayes Classifier Essentials >>* http://www.sthda.com/english/articles/36-classification-methods-essentials/145-naive-bayes-classifier-essentials/
    * **Logistic regression >>** 
      * https://www.datacamp.com/community/tutorials/logistic-regression-R
      * https://www.r-bloggers.com/evaluating-logistic-regression-models/
  * **Online tools >>** 
    * https://www.google.com/search?q=color+picker
  * **Technical troubleshooting >>** https://stackoverflow.com/
    * https://stackoverflow.com/questions/8499361/easy-way-of-counting-precision-recall-and-f1-score-in-r
    * **gtable >>** https://www.rdocumentation.org/packages/gt/versions/0.1.0




