---
title: "Example HR research using interactive R Markdown"
date: "February 4, 2019"
author: "Jeff Idle"
output: html_notebook
runtime: shiny
editor_options: 
  chunk_output_type: inline
---

# Purpose and Background
The purpose of this analysis is to showcase the capabilities of interactive R Markdown documents. R Markdown documents are great for integrating analysis with visualizations. Interactive R Markdown documents provide a better way to engage your stakeholders and let them explore your analysis further. IBM posted a sample Human Resources Employee Attrition and Performance dataset on their [Watson Analytics Community web site](https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/). The imaginary stakeholders for this example analysis will be the CEO and HR VP for a fictional medium-sized company. Our fictional company researches and designs productivity and entertainment peripherals that can be biologically integrated with the human body and connect to the IoT (Internet of Things). Our fictional company is called "Shadowrun, Inc.". The dataset does not contain hire dates or termination dates, so for purposes of this analysis we will assume all of the attrition in our dataset represents voluntary turnover for the last five years. The source code for this analysis can be found on Github at https://github.com/jeffidle/hr_attrition_example.

**NOTE:**  Please note that this analysis is an example for illustrative purposes only. As such, this analysis is not guaranteed to be error-free or copied without further analysis and modifications to fit the situation to which it is being applied. 


# Business Questions
Our analytics team met with Shadow Run's CEO and HR VP to discuss their critical workforce planning and management questions. They identified and prioritized research into the following three questions:

  1. At what levels of service with the company do we have the greatest risk of losing employees?
  2. What are the key drivers of Shadow Run's turnover?
  3. Can we develop a model to predict the risk of losing our currently active employees?


# Executive Summary
Our analytics team found the following:

  * **High risk levels of service >>** TBD
  * **Voluntary turnover drivers >>** TBD
  * **Turnover risk model >>** TBD


# Methodologies
We utilized several methodologies in answering the business questions. Following are the methods we applied for each business question.

  * **Retention risk by years of service >>** Survival analysis
  * **Voluntary turnover drivers >>** Random forest & logistic regression
  * **Retention risk model >>** Machine learning, logistic regression, naive Bayes, random forest & regression trees


# Analysis
The analysis that follows will start with exploratory analyses to understand the data. We will then analyze the data to answer the question about years of service where retention risk is greatest. Next we will analyze the key drivers that are highly correlated with turnover during the last 5 years. Finally, we will create a predictive model that enables us to assess the risk of losing our currently active employees and estimate the risk of losing any new employees that join the organization.

```{r echo = FALSE, message=FALSE, warning=FALSE}

library(tidyverse)
library(shiny)
library(survival)
library(survminer)
library(ggfortify)
library(scales)
library(lubridate)
library(caret)
library(randomForest)
library(ipred)
library(gbm)
library(broom)
library(rlang)
library(gridExtra)
library(klaR)
library(rpart)
library(gt)

```

```{r echo = FALSE, message=FALSE, warning=FALSE}

color_test_df <- data.frame(
        
        letter = letters[1:10],
        number = seq(11:20)
        
)

custom_palette <- c("#000000",
                    "#8f8f8f",
                    "#5378b5",
                    "#f46242",
                    "#1bb764",
                    "#cc4b4b",
                    "#8a32c1",
                    "#398ed3",
                    "#aa9620",
                    "#d83aae")

color_test_viz <- ggplot(color_test_df, aes(x = letter, y = number, fill = letter)) +
        geom_bar(stat = "identity") +
        scale_fill_manual(values = custom_palette) +
        coord_flip() +
        theme_minimal()

#color_test_viz

```


```{r echo = FALSE, message=FALSE, warning=FALSE}

attrition_df <- read.csv("data/ibm_attrition.csv", stringsAsFactors = FALSE)
attrition_ref_data_df <- read.csv("data/ibm_attrition_ref_data.csv", stringsAsFactors = FALSE)
#training_df <- read.csv("data/ibm_training.csv", stringsAsFactors = FALSE)

```

```{r echo = FALSE, message=FALSE, warning=FALSE}

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "Education", 2:3], by.x = "Education", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "EducationText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "EnvironmentSatisfaction", 2:3], by.x = "EnvironmentSatisfaction", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "EnvironmentSatisfactionText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "JobInvolvement", 2:3], by.x = "JobInvolvement", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "JobInvolvementText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "JobSatisfaction", 2:3], by.x = "JobSatisfaction", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "JobSatisfactionText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "PerformanceRating", 2:3], by.x = "PerformanceRating", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "PerformanceRatingText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "RelationshipSatisfaction", 2:3], by.x = "RelationshipSatisfaction", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "RelationshipSatisfactionText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "WorkLifeBalance", 2:3], by.x = "WorkLifeBalance", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "WorkLifeBalanceText"

attrition_df <- attrition_df %>%
        mutate(terminated = ifelse(Attrition == "Yes", 1, 0))

attrition_df <- as.data.frame(unclass(attrition_df))

attrition_data_df <- attrition_df[ , c(16:42, 8:14, 43)]
attrition_data_df$terminated <- factor(attrition_data_df$terminated)
attrition_df$Attrition <- factor(attrition_df$Attrition, levels = c("Yes", "No"))

```

```{r echo = FALSE, message=FALSE, warning=FALSE}

options(scipen = 999)

data_summary_df <- data.frame(
        
        data_element = names(attrition_df),
        data_type = unlist(lapply(attrition_df, class))
        
)

numeric_cols_df <- attrition_df %>% select_if(is.numeric)
integer_cols_df <- attrition_df %>% select_if(is.integer)
num_int_cols_df <- bind_cols(numeric_cols_df, integer_cols_df)
factor_cols_df <- attrition_df %>% select_if(is.factor)
character_cols_df <- attrition_df %>% select_if(is.character)

data_levels <- lapply(factor_cols_df, levels)

min_vals <- lapply(num_int_cols_df, min)
mean_vals <- lapply(num_int_cols_df, mean)
max_vals <- lapply(num_int_cols_df, max)
col_count <- length(data_levels)
col_count2 <- length(min_vals)

for(i in 1:col_count){
        
        tmp_level_df <- data.frame(
                
                data_element = names(data_levels[i]),
                data_level_count = paste0(length(data_levels[[i]]), collapse = ""),
                data_level_text = paste0(data_levels[[i]], collapse = ", ")
                
        )
        
        if(!exists("level_df")){
                
               level_df <- tmp_level_df 
                
        } else level_df <- bind_rows(level_df, tmp_level_df)
        
}

level_df <- level_df %>% mutate(data_summary = paste0("level count = ", data_level_count, "; levels = ", data_level_text))

for(i in 1:col_count2){
        
        tmp_numsum_df <- data.frame(
                
                data_element = names(min_vals[i]),
                min_value = min_vals[[i]],
                mean_value = mean_vals[[i]],
                max_value = max_vals[[i]]
                
        )
        
        if(!exists("numsum_df")){
                
               numsum_df <- tmp_numsum_df 
                
        } else numsum_df <- bind_rows(numsum_df, tmp_numsum_df)
        
}

numsum_df <- numsum_df %>% mutate(data_summary = paste0("min = ", min_value, "; mean = ", round(mean_value, 3), "; max = ", max_value))

col_summaries_df <- bind_rows(level_df[ , c(1, 4)], numsum_df[ , c(1, 5)])

data_summary_df <- merge(data_summary_df, col_summaries_df, by = "data_element", all.x = TRUE, all.y = FALSE)

data_summary_df <- data_summary_df %>%
        arrange(data_type, data_element)

#rm(tmp_numsum_df, numsum_df, tmp_level_df, level_df, data_levels, data_summary_df)

```




#### Exploratory analysis
The starting point for every analysis is to become familiar with the data. You can view the raw data [here](https://github.com/jeffidle/hr_attrition_example/blob/master/data/ibm_attrition.csv) but let's take a look at a summarized view of the data. 

```{r echo = FALSE, message=FALSE, warning=FALSE}

data_summary_df %>% gt() %>%
        tab_options(table.font.size = 10,
                    row.padding = px(1)
                    )

```




Now, let's visualize the data. We would first like to see how are data is distributed among active and terminated records. The following chart shows that 16% of the records in our dataset are voluntary terminations and the remaining 84% are currently active employees.

```{r echo = FALSE, message=FALSE, warning=FALSE}

term_dist_viz <- ggplot(attrition_df, aes(x = Attrition, fill = Attrition)) +
        geom_bar() +
        geom_text(stat = "count", aes(label = ..count..), hjust = -0.5) +
        scale_fill_manual(values = c("#ED8800", "#5378b5")) +
        labs(title = "Employee count by attrition category", subtitle = "(no = currently active; yes = terminated)", x = "", y = "") +
        coord_flip() +
        theme(panel.background = element_blank(),
              axis.line.x = element_blank(),
              axis.line.y = element_blank(),
              axis.ticks.x = element_blank(),
              axis.ticks.y = element_blank(),
              axis.text.x = element_blank(),
              legend.position = "none")

renderPlot({
  term_dist_viz
})

```


The drop-down box below allows you to view the counts of employees by the selected dimension and the distribution of active versus terminated employee records within the levels of that dimension. Feel free to use the drop-down to explore the data further.

```{r echo = FALSE}

selectInput(inputId = "explore_dim", label = "Choose the variable that you would like to explore:",
            choices = c("Business Travel", "Department", "Education", "Education Field", "Gender", "Job Involvement", "Job Role",
                        "Job Satisfaction", "Marital Status", "Overtime", "Peformance Rating", "Relationship Satisfaction",
                        "Stock Options Level", "Work/Life Balance"), selected = "Gender")

dim1_select <- reactive({
        switch(input$explore_dim,
               "Business Travel" = attrition_df$BusinessTravel,
               "Department" = attrition_df$Department,
               "Education" = attrition_df$EducationText,
               "Education Field" = attrition_df$EducationField,
               "Gender" = attrition_df$Gender,
               "Job Involvement" = attrition_df$JobInvolvementText,
               "Job Role" = attrition_df$JobRole,
               "Job Satisfaction" = attrition_df$JobSatisfactionText,
               "Marital Status" = attrition_df$MaritalStatus,
               "Overtime" = attrition_df$OverTime,
               "Performance Rating" = attrition_df$PerformanceRatingText,
               "Stock Options Level" = attrition_df$StockOptionLevel,
               "Work/Life Balance" = attrition_df$WorkLifeBalanceText)
        })

renderPlot({
        
        explore_dist_viz <- ggplot(attrition_df, aes(x = dim1_select())) +
                geom_bar(fill = "#8f8f8f") +
                geom_text(stat = "count", aes(label = ..count..), hjust = 1.5, color = "#ffffff") +
                labs(title = paste0("Employee count distribution"), x = "", y = "") +
                coord_flip() +
                theme(panel.background = element_blank(),
                      axis.line.x = element_blank(),
                      axis.line.y = element_blank(),
                      axis.ticks.x = element_blank(),
                      axis.ticks.y = element_blank(),
                      axis.text.x = element_blank(),
                      legend.position = "none")
        
        explore2_dist_viz <- ggplot(attrition_df, aes(x = dim1_select(), y = EmployeeCount, fill = Attrition)) +
                geom_bar(position = "fill", stat = "identity") +
                labs(title = paste0("Active versus terminated distribution"), x = "", y = "") +
                scale_fill_manual(values = c("#ED8800", "#5378b5")) +
                scale_y_continuous(label = percent_format()) +
                coord_flip() +
                theme(panel.background = element_blank(),
                      axis.line.y = element_blank(),
                      axis.ticks.y = element_blank())
        
        explore2_dist_viz
        
        grid.arrange(explore_dist_viz, explore2_dist_viz, ncol = 2)
        
})

```


#### Retention Risk by Years of Service
Understanding whether there are years throughout an employee's tenure in which retention risk is highest can help Shadowrun create focused and targeted retention programs. Targeted programs should be more effective than one-size-fits-all programs because targeted programs are tailored to the specific needs of the population at risk. By leveraging the survival analysis methodology in the context of employee attrition, we can assess the average drop in retention probability for each year of service to identify those years in which retention risk is greatest.

The chart on the left shows how the probability of retaining Shadowrun employees drops as tenure increases. This is expected to some extent--everyone will retire someday and that is part of voluntary termination. If all Shadowrun employees were hired directly from high school or college and stayed until they retired, then the "survival curve" would bow outward to the right. If employees were leaving within relatively short periods of time, then the "survival curve" would bow inward to the left. Shadowrun's curve bows outward to the right, which tells us that our employees tend to stay with us. In fact, our employees have a 25% probability of staying with us for 15 years and 40% probability of staying with us for 31 years. The first half of the curve is generally smooth, which tells us there isn't a lot of year-over-year variation. The jagged portions at the end of the curve exist simply because there is a very small population of employees at those tenure levels. 

The chart on the right shows the average reduction in retention probability for each year of service. This chart focuses on the first 20 years since that constitutes the bulk of our workforce. The overall average decrease in retention probability for each year of service is 1.4% if we exclude the outlier in year 40. We can see in years 1 and 10 that the decrease is around 4%--nearly 3x the average. We should work with Shadowrun's HR Business Partners to better understand what might be going on in these years. The first year spike is not unexpected and may be due to onboarding, poor culutural fit, or other factors common to employees accepting a new job at a new company. The tenth-year spike, however, is unusual and should be investigated further.

```{r echo = FALSE}

surv_object <- Surv(time = attrition_df$YearsAtCompany, event = attrition_df$terminated)

fit1 <- survfit(surv_object ~ 1, data = attrition_df)

model_df <- fortify(fit1)

model_top20_df <- model_df %>%
        arrange(time) %>%
        mutate(avg_probability_decrease = lag(surv) - surv) %>%
        filter(time < 40 & time > 0)

mean_rr <- mean(model_top20_df$avg_probability_decrease, na.rm = TRUE)
sd_rr <- sd(model_top20_df$avg_probability_decrease, na.rm = TRUE)
sd15_top_rr <- mean_rr + (1.5 * sd_rr)

model_top20_df <- model_top20_df %>%
        mutate(risk_increase_size = ifelse(avg_probability_decrease >= sd15_top_rr, ">= 1.5 standard deviations", "< 1.5 standard deviations")) %>%
        filter(time <= 20 & time > 0)

survival_viz <- ggplot(model_df, aes(x = time, y = surv)) +
        geom_line(size = 1.25) +
        scale_y_continuous(label = percent_format()) +
        labs(title = "Retention probability analysis", subtitle = "", x = "years at company", y = "retention probability") +
        theme_minimal()

risk_viz <- ggplot(model_top20_df, aes(x = reorder(time, desc(time)), y = avg_probability_decrease, fill = risk_increase_size)) +
        geom_bar(stat = "identity") +
        scale_fill_manual(values = c("#798396", "#ED8800")) +
        scale_y_continuous(label = percent_format()) +
        geom_hline(yintercept = mean_rr, color = "#8F8F8F", size = 2, alpha = 0.5) +
        annotate("text", label = "- overall average = 1.4%", x = 5, y = 0.016, vjust = 0, hjust = 0.005) +
        coord_flip() +
        labs(title = "Increase in retention risk for each year of service", subtitle = "", x = "tenure (years)", y = "average retention risk increase") +
        theme_minimal()

renderPlot({
  grid.arrange(survival_viz, risk_viz, ncol = 2)
})

```



Overall, we don't have major concerns about retention risk by tenure, but we should investigate the spikes in years 1 and 10. We would be surprised if there is not a policy, program or issue that explains why more employees leave in their 10th year than normal. What we don't know, however, is whether there are segments of the population where there are more clear patterns in higher retention risk by tenure. Use the drop-down below to explore various dimensions. If you slice by Education Field you will notice the employees whose education is in Human Resources have a high retention risk increases in years 2 - 4.

```{r echo = FALSE}

selectInput(inputId = "survival_slice", label = "Choose the dimension by which you'd like to slice the retention analysis:", 
  choices = c("Business Travel", "Department", "Education", "Education Field", "Gender", "Job Involvement", "Job Role",
                        "Job Satisfaction", "Marital Status", "Overtime", "Peformance Rating", "Relationship Satisfaction",
                        "Stock Options Level", "Work/Life Balance"), selected = "Gender")

dimension_choice <- reactive({
        switch(input$survival_slice,
               "Business Travel" = attrition_df$BusinessTravel,
               "Department" = attrition_df$Department,
               "Education" = attrition_df$EducationText,
               "Education Field" = attrition_df$EducationField,
               "Gender" = attrition_df$Gender,
               "Job Involvement" = attrition_df$JobInvolvementText,
               "Job Role" = attrition_df$JobRole,
               "Job Satisfaction" = attrition_df$JobSatisfactionText,
               "Marital Status" = attrition_df$MaritalStatus,
               "Overtime" = attrition_df$OverTime,
               "Performance Rating" = attrition_df$PerformanceRatingText,
               "Stock Options Level" = attrition_df$StockOptionLevel,
               "Work/Life Balance" = attrition_df$WorkLifeBalanceText)
        })

renderPlot({
        
        fit2 <- survfit(surv_object ~ dimension_choice(), data = attrition_df)
        
        model2_df <- fortify(fit2)
        
        model2_top20_df <- model2_df %>%
                arrange(strata, time) %>%
                mutate(avg_probability_decrease = lag(surv) - surv) %>%
                filter(time < 40 & time > 0)
        
        mean_rr2 <- mean(model2_top20_df$avg_probability_decrease, na.rm = TRUE)
        sd_rr2 <- sd(model2_top20_df$avg_probability_decrease, na.rm = TRUE)
        sd15_top_rr2 <- mean_rr2 + (1.5 * sd_rr2)
        
        model2_top20_df <- model2_top20_df %>%
                mutate(risk_increase_size = ifelse(avg_probability_decrease >= sd15_top_rr, ">= 1.5 standard deviations", "< 1.5 standard deviations")) %>%
                filter(time <= 20 & time > 0)
        
        level_count <- length(model2_df$strata)
        
        survival2_viz <- ggplot(model2_df, aes(x = time, y = surv, color = strata)) +
                geom_line(size = 1.25) +
                scale_color_manual(values = custom_palette[1:level_count]) +
                scale_y_continuous(label = percent_format()) +
                labs(title = "Retention probability analysis", subtitle = "", x = "years at company", y = "retention probability") +
                theme_minimal()
        
        risk2_viz <- ggplot(model2_top20_df, aes(x = reorder(time, desc(time)), y = avg_probability_decrease, fill = risk_increase_size)) +
                geom_bar(stat = "identity") +
                geom_hline(yintercept = mean_rr2, color = "#8F8F8F", size = 2, alpha = 0.5) +
                scale_fill_manual(values = c("#798396", "#ED8800")) +
                scale_y_continuous(label = percent_format()) +
                facet_wrap(~strata) +
                coord_flip() +
                labs(title = "Increase in retention risk for each year of service", subtitle = paste0("(overall average = ", percent(mean_rr2, 3), ")"), x = "tenure (years)", y = "average retention risk increase") +
                theme(panel.background = element_blank(),
                      panel.grid.major.x = element_line(color = "#dee2e8"))
        
        grid.arrange(survival2_viz, risk2_viz, ncol = 2)
        
        })

```

#### Driver Analysis

Understanding the factors that are more highly correlated with attrition can help us investigate root causes and develop programs, policies or processes to address those root causes. Note that **correlation is not causation**. If we find correlations between a factor and attrition, we know that there is a relationship *but we don't know why* there is a relationship. For example, if there is a high correlation between lower Employee Satisfaction Survey scores and attrition, then we know that employees who leave may tend to have lower Employee Satisfaction Survey scores, but we don't know specifically why the employees left. The reasons could vary widely and require further root cause investigation.

We will apply a two-step process to find the drivers of Shadowrun's attrition. We will apply an initial first-pass methodology to determine which factors may be the most important. We will then apply a second-pass methodology to determine which levels within each factor are the most significant drivers of attrition. For example, our first pass may find that the Job Description is an important factor. Knowing that the employee's job is a factor in attrition is important, but it is not helpful unless we know which jobs have the greatest impact on attrition.

The factor importance chart below show us the relative importance of all of the factors in our dataset. This first pass applies a random forest algorithm to compute the relative importance of each factor. Note that you will get different results depending on the algorithm or method you choose.

```{r echo = FALSE, message=FALSE, warning=FALSE}

driver_data_df <- attrition_data_df

# Remove StandardHours and Over18 since there is no variation and remove Attrition since it is redundant
driver_data_df <- driver_data_df[ , -c(10, 13, 29)]

model_driver_rf <- randomForest(terminated ~ ., data = driver_data_df[, -1])
driver_imp_rf <- varImp(model_driver_rf)
driver_imp_rf$driver <- row.names(driver_imp_rf)
driver_imp_rf <- driver_imp_rf[ , c(2, 1)] %>%
        arrange(desc(Overall))

renderPlot({
        
        variable_viz <- ggplot(driver_imp_rf, aes(x=reorder(driver, Overall), y = Overall)) +
                geom_point() +
                geom_segment(aes(x=driver,xend=driver,y=0,yend=Overall)) +
                labs(title = "Shadowrun attrition factor importance", subtitle = "(generated by random forest model)", x = "", y = "mean decrease in Gini") +
                theme(panel.background = element_blank(),
                      panel.grid.major.x = element_line(color = "#dee2e8")) +
                coord_flip()
        
        variable_viz
        
        })

```


Now that we know the relative importance of each factor, the next step is to decide which factors to include in our model. The drop-down below allows you to select the Importance threshold for those factors you want to include in the model. After you choose your desired threshold, we apply the second-pass step of including the selected factors in a logistic regression model. The output table that you will see when you select your threshold includes the output from the model. The coefficient column tells us whether the relationship between that factor level and attrition is positive or negative and the degree to which they are related. Values at or near zero have very little influence on attrition. The p-value column tells us whether the relationship is significant. For purposes of our analysis we will use a significance level of 0.05 (p <= 0.5) but it is important to note that there are [debates in the scientific community](https://www.nature.com/articles/s41562-017-0189-z) swelling around the appropriate significance level to use. 

```{r echo = FALSE, message=FALSE, warning=FALSE}

# Set Gini index cutoff

numericInput(inputId = "gini_cutoff", label = "Choose the Gini cut-off point below which you want to exclude variables:",
            min = min(driver_imp_rf$Overall), max = max(driver_imp_rf$Overall), value = 15)

render_gt({
        
        options(scipen = 999)
        
        variables_chosen <- driver_imp_rf %>% filter(Overall >= input$gini_cutoff) %>% dplyr::select(driver)
        
        #variables_chosen <- driver_imp_rf %>% filter(Overall >= 1) %>% dplyr::select(driver)
        
        variables_chosen <- as.vector(variables_chosen$driver)
        
        driver_data_f_df <- driver_data_df %>% dplyr::select(EmployeeNumber, variables_chosen, terminated)
        
        model_driver_lr <- glm(terminated ~ ., data = driver_data_f_df[, -1], family = "binomial")
        
        model_driver_lr_df <- tidy(model_driver_lr)
        
        model_driver_lr_df <- model_driver_lr_df %>%
                dplyr::select(term, estimate, p.value) %>%
                mutate(usefulness_category = ifelse(p.value > 0.05, "not significant (p-value > 0.05)",
                                                    ifelse(p.value <= 0.05 & abs(estimate) > 0.1, "significant and meaningful (p-value <= 0.05 and coefficient > 0.1 or < -0.1)", 
                                                    ifelse(p.value <= 0.05 & abs(estimate) <= 0.1, "significant but not meaningful (p-value <= 0.05 and coefficient between 0.1 and -0.1)", "unknown"))),
                       usefulness_cat_num = ifelse(p.value > 0.05, -1,
                                                    ifelse(p.value <= 0.05 & abs(estimate) > 0.1, 1, 
                                                    ifelse(p.value <= 0.05 & abs(estimate) <= 0.1, 0, 0)))) %>%
                arrange(desc(usefulness_cat_num), desc(estimate), term) %>%
                mutate(estimate = round(estimate, 3),
                       p.value = round(p.value, 4)) %>%
                dplyr::select(-usefulness_cat_num)
        
        names(model_driver_lr_df) <- c("factor", "coefficient", "p.value", "usefulness_category")
                
        g_table <- model_driver_lr_df %>%
                gt(groupname_col = "usefulness_category") %>%
                tab_options(table.font.size = 10,
                    row.padding = px(1)
                    )
        
        g_table
        
})

```


# Retention Risk Scoring
Now that we have analyzed which factors and factor levels are correlated with attrition and the direction and strength of that relationship, let's leverage that data to assist with decision-making and more focused analysis. We will develop a more fine-tuned model using machine learning techniques to predict which of our active employees may be at risk of leaving. The general process we follow to build a machine learning model is:

  1. data collection;
  2. data preparation;
  3. building the model;
  4. training the model;
  5. evaluating model performance; and
  6. improving model performance.

### Data Collection and Preparation

We have already completed a large portion of data collection and preparation for the previous analyses. A data collection and preparation and processing step that is unique to machine learning is to split data into "training" and "test" datasets. The training dataset is used to build the model and the test dataset is used to evaluate the accuracy and precision of the model. The split is usually a percentage split with the larger percentage being put in the training dataset and the smaller percentage put into the test dataset. Common splits are 70%/30% and 80%/20%. We will use an 80/20 split for our analysis.


```{r echo = FALSE, message=FALSE, warning=FALSE}

# numericInput(inputId = "train_split", label = "Choose the percentage of data you would like to use for your training dataset:  ", value = .80, min = .01, max = 0.99, step = .05)

modeling_data_df <- attrition_data_df

# Remove StandardHours and Over18 since there is no variation and remove Attrition since it is redundant
modeling_data_df <- modeling_data_df[ , -c(10, 13, 35)]
modeling_data_df$Attrition <- as.character(modeling_data_df$Attrition)
modeling_data_df <- modeling_data_df %>% 
        dplyr::select(-Attrition, Attrition)

modeling_data_df$Attrition[modeling_data_df$Attrition == "No"] <- "active"
modeling_data_df$Attrition[modeling_data_df$Attrition == "Yes"] <- "termed"
modeling_data_df$Attrition <- factor(modeling_data_df$Attrition, levels = c("active", "termed"))

set.seed(1)

# Split the data into training and test set
training_samples_df <- modeling_data_df$EmployeeNumber %>%
        createDataPartition(p = 0.8, list = FALSE)  

train_data_df  <- modeling_data_df[training_samples_df, ]
test_data_df <- modeling_data_df[-training_samples_df, ]

```


### Build and Train Models

When building a machine learning model, it can be beneficial to try a few different algorithms and compare their performance. For purposes of this illustration, we will build models using the naive Bayes, logistic regression, decision tree and random forest algorithms. We will not discuss these algorithms in detail. A confusion matrix is an important tool in helping assess the performance of a model. Several metrics can be generated from a confusion matrix. These metrics help us decide whether the model or models will work for our purposes. Following are the confusion matrices for each of the models we created.

```{r echo = FALSE, message=FALSE, warning=FALSE}

set.seed(1)

# Fit the models
model_nb <- NaiveBayes(Attrition ~ ., data = train_data_df[, -1])
model_lr <- glm(Attrition ~ ., data = train_data_df[, -1], family = "binomial")
model_rf <- randomForest(Attrition ~ ., data = train_data_df[, -1])
model_rt <- rpart(Attrition ~ ., data = train_data_df[, -1])

# Make predictions
predictions_nb <- model_nb %>% predict(test_data_df)
predictions_lr_resp <- model_lr %>% predict(test_data_df, type = "response")
predictions_lr <- factor(ifelse(predictions_lr_resp > 0.5, "termed", "active"), levels = c("active", "termed"))
predictions_rf <- model_rf %>% predict(test_data_df)
predictions_rt <- model_rt %>% predict(test_data_df, type = "class")

# Model accuracies
model_build_accuracy_nb <- mean(predictions_nb$class == test_data_df$Attrition)
model_build_accuracy_lr <- mean(predictions_lr == test_data_df$Attrition)
model_build_accuracy_rf <- mean(predictions_rf == test_data_df$Attrition)
model_build_accuracy_rt <- mean(predictions_rt == test_data_df$Attrition)

# Confusion matrices
model_build_cm_nb <- confusionMatrix(predictions_nb$class, test_data_df$Attrition)
model_build_cm_lr <- confusionMatrix(predictions_lr, test_data_df$Attrition)
model_build_cm_rf <- confusionMatrix(predictions_rf, test_data_df$Attrition)
model_build_cm_rt <- confusionMatrix(predictions_rt, test_data_df$Attrition)

cm_nb <- data.frame(as.matrix(model_build_cm_nb$table)) %>%
        mutate(algorithm = "naive_bayes")

cm_lr <- data.frame(as.matrix(model_build_cm_lr$table)) %>%
        mutate(algorithm = "logistic_regression")

cm_rf <- data.frame(as.matrix(model_build_cm_rf$table)) %>%
        mutate(algorithm = "random_forest")

cm_rt <- data.frame(as.matrix(model_build_cm_rt$table)) %>%
        mutate(algorithm = "regression_tree")



```


```{r echo = FALSE, message=FALSE, warning=FALSE}

# Model performance metrics
# F-Score: (2 * Precision * Recall)/(Precision + Recall) harmonic mean of precision and recall

metric_labels <- row.names(as.data.frame(c(model_build_cm_nb$overall, model_build_cm_nb$byClass)))

perf_metrics_nb_untrained <- data.frame(metric = metric_labels,
                                        metric_result = c(model_build_cm_nb$overall, model_build_cm_nb$byClass)) %>%
        mutate(algorithm = "naive_bayes",
               phase_category = "untrained")

perf_metrics_lr_untrained <- data.frame(metric = metric_labels,
                                        metric_result = c(model_build_cm_lr$overall, model_build_cm_lr$byClass)) %>%
        mutate(algorithm = "logistic_regression",
               phase_category = "untrained")

perf_metrics_rf_untrained <- data.frame(metric = metric_labels,
                                        metric_result = c(model_build_cm_rf$overall, model_build_cm_rf$byClass)) %>%
        mutate(algorithm = "random_forest",
               phase_category = "untrained")

perf_metrics_rt_untrained <- data.frame(metric = metric_labels,
                                        metric_result = c(model_build_cm_rt$overall, model_build_cm_rt$byClass)) %>%
        mutate(algorithm = "regression_tree",
               phase_category = "untrained")

perf_metrics_untrained_df <- bind_rows(perf_metrics_nb_untrained, perf_metrics_lr_untrained, perf_metrics_rf_untrained,
                                       perf_metrics_rt_untrained)

perf_metrics_untrained_df %>%
        gt(group = "algorithm") %>%
        fmt_percent(2, decimals = 1) %>%
        tab_options(table.font.size = 10,
                    row.padding = px(1)
                    )

```



### Evaluate Model Performance



```{r echo = FALSE, message=FALSE, warning=FALSE}

set.seed(1)

# Build the models
model_test_nb_df <- train(terminated ~ ., data = train_data_df[, -1], method = "nb",
                          trControl = trainControl("cv", number = 10))
model_test_lr_df <- train(terminated ~ ., data = train_data_df[, -1], method = "glm", family = "binomial",
                          trControl = trainControl("cv", number = 10))
model_test_rf_df <- train(terminated ~ ., data = train_data_df[, -1], method = "rf",
                          trControl = trainControl("cv", number = 10))
model_test_rt_df <- train(terminated ~ ., data = train_data_df[, -1], method = "rpart",
                          trControl = trainControl("cv", number = 10))

# Make predictions
predicted_classes_nb <- model_test_nb_df %>% predict(test_data_df)
predicted_classes_lr <- model_test_lr_df %>% predict(test_data_df)
predicted_classes_rf <- model_test_rf_df %>% predict(test_data_df)
predicted_classes_rt <- model_test_rt_df %>% predict(test_data_df)

# Calculate the confusion matrices
model_trained_cm_nb <- confusionMatrix(predicted_classes_nb, test_data_df$terminated)
model_trained_cm_lr <- confusionMatrix(predicted_classes_lr, test_data_df$terminated)
model_trained_cm_rf <- confusionMatrix(predicted_classes_rf, test_data_df$terminated)
model_trained_cm_rt <- confusionMatrix(predicted_classes_rt, test_data_df$terminated)

# Model validation metrics
# F-Score: (2 * Precision * Recall)/(Precision + Recall) harmonic mean of precision and recall
perf_metrics_nb_trained <- as.data.frame(c(model_trained_cm_nb$overall, model_trained_cm_nb$byClass))
perf_metrics_lr_trained <- as.data.frame(c(model_trained_cm_lr$overall, model_trained_cm_lr$byClass))
perf_metrics_rf_trained <- as.data.frame(c(model_trained_cm_rf$overall, model_trained_cm_rf$byClass))
perf_metrics_rt_trained <- as.data.frame(c(model_trained_cm_rt$overall, model_trained_cm_rt$byClass))

metric_labels <- row.names(perf_metrics_nb_trained)

perf_metrics_trained_df <- bind_cols(perf_metrics_nb_trained, perf_metrics_lr_trained, perf_metrics_rf_trained, perf_metrics_rt_trained)
names(perf_metrics_trained_df) <- c("naive_bayes_trained", "logistic_regression_trained", "random_forest_trained", "regression_tree_trained")
perf_metrics_trained_df$metric <- metric_labels

perf_metrics_trained_df <- perf_metrics_trained_df[ , c(5, 2, 1, 3:4)]

perf_metrics_trained_df %>%
        gt() %>%
        fmt_percent(c(2:5), decimals = 1) %>%
        tab_options(table.font.size = 10,
                    row.padding = px(1)
                    )

```


### Improve Model Performance


```{r echo = FALSE, message=FALSE, warning=FALSE}

set.seed(1)

options(scipen = 999)

feature_select_lr <- varImp(model_test_lr_df)$importance

```



```{r echo = FALSE, message=FALSE, warning=FALSE}

set.seed(1)

options(scipen = 999)

varImp(model_test_lr_df)
varImp(model_test_nb_df)
varImp(model_test_rf_df)
varImp(model_test_rt_df)

model_input_data_df <- attrition_data_df[ , c(2:9, 11:28, 30:35)]

fit_rf <- randomForest(terminated ~ ., data = model_input_data_df)

# Create an importance based on mean decreasing gini
gini_df <- as.data.frame(importance(fit_rf))

```



# Conclusion


# References and Resources
Following are the online articles and resources I used in compiling this analysis:

  * **Employee attrition articles and references >>**
    * *Extended Tutorial: How to Predict Employee Turnover >>* https://www.hranalytics101.com/extended-tutorial-how-to-predict-employee-turnover/
  * **Sample attrition data posted by IBM >>** https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/
  * **Interactive Shiny documents >>**
    * https://bookdown.org/yihui/rmarkdown/shiny-documents.html
    * https://rmarkdown.rstudio.com/lesson-14.html
  * **Survival analysis >>** https://www.datacamp.com/community/tutorials/survival-analysis-R
  * **Feature selection >>** 
    * https://www.r-bloggers.com/feature-selection-using-the-caret-package/
    * http://dataaspirant.com/2018/01/15/feature-selection-techniques-r/
    * https://www.r-bloggers.com/variable-importance-plot-and-variable-selection/
    * https://www.datacamp.com/community/tutorials/feature-selection-R-boruta
    * https://freakonometrics.hypotheses.org/19835
    * https://blog.datadive.net/selecting-good-features-part-iii-random-forests/
  * **Machine Learning/Predictive Modeling >>**
      *Machine Learning with R*, (2013) by Brett Lantz
      * https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7
    * **Naive Bayes >> ** 
      * https://www.r-bloggers.com/understanding-naive-bayes-classifier-using-r/
      * *Naive Bayes Classifier Essentials >>* http://www.sthda.com/english/articles/36-classification-methods-essentials/145-naive-bayes-classifier-essentials/
    * **Logistic regression >>** 
      * https://www.datacamp.com/community/tutorials/logistic-regression-R
      * https://www.r-bloggers.com/evaluating-logistic-regression-models/
  * **Online tools >>** 
    * https://www.google.com/search?q=color+picker
  * **Technical troubleshooting >>** https://stackoverflow.com/
    * https://stackoverflow.com/questions/8499361/easy-way-of-counting-precision-recall-and-f1-score-in-r
    * **gtable >>** 
      * https://github.com/rstudio/gt/blob/master/README.md
      * https://www.rdocumentation.org/packages/gt/versions/0.1.0




