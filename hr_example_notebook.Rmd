---
title: "Example HR research using interactive R Markdown"
date: "February 4, 2019"
author: "Jeff Idle"
output: html_notebook
runtime: shiny
editor_options: 
  chunk_output_type: inline
---

# Purpose and Background
The purpose of this analysis is to showcase the capabilities of interactive R Markdown documents. R Markdown documents are great for integrating analysis with visualizations. Interactive R Markdown documents provide a better way to engage your stakeholders and let them explore your analysis further. IBM posted a sample Human Resources Employee Attrition and Performance dataset on their [Watson Analytics Community web site](https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/). The imaginary stakeholders for this example analysis will be the CEO and HR VP for a fictional medium-sized company. Our fictional company researches and designs productivity and entertainment peripherals that can be biologically integrated with the human body and connect to the IoT (Internet of Things). Our fictional company is called "Shadow Run, Inc.". For purposes of this analysis, we will assume all of the attrition in our dataset represents voluntary turnover. The source code for this analysis can be found on Github at https://github.com/jeffidle/hr_attrition_example.


# Business Questions
Our analytics team met with Shadow Run's CEO and HR VP to discuss their critical workforce planning and management questions. They identified and prioritized research into the following three questions:

  1. At what levels of service with the company do we have the greatest risk of losing employees?
  2. What are the key drivers of Shadow Run's turnover?
  3. Can we develop a model to predict the risk of losing our currently active employees?


# Executive Summary
Our analytics team found the following:
  * **High risk levels of service >>** TBD
  * **Voluntary turnover drivers >>** TBD
  * **Turnover risk model >>** TBD


# Methodologies
  * Survival analysis
  * Logistic regression
  * Machine learning


# Analysis
The starting point for every analysis is to become familiar with the data. 

```{r echo = FALSE, message=FALSE, warning=FALSE}

library(tidyverse)
library(shiny)
library(survival)
library(survminer)
library(ggfortify)
library(scales)
library(lubridate)
library(caret)
library(randomForest)
library(ipred)
library(gbm)
library(broom)
library(rlang)
library(gridExtra)
library(klaR)
library(rpart)
library(gt)

```

```{r echo = FALSE, message=FALSE, warning=FALSE}

attrition_df <- read.csv("data/ibm_attrition.csv", stringsAsFactors = FALSE)
attrition_ref_data_df <- read.csv("data/ibm_attrition_ref_data.csv", stringsAsFactors = FALSE)
#training_df <- read.csv("data/ibm_training.csv", stringsAsFactors = FALSE)

```

```{r echo = FALSE, message=FALSE, warning=FALSE}

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "Education", 2:3], by.x = "Education", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "EducationText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "EnvironmentSatisfaction", 2:3], by.x = "EnvironmentSatisfaction", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "EnvironmentSatisfactionText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "JobInvolvement", 2:3], by.x = "JobInvolvement", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "JobInvolvementText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "JobSatisfaction", 2:3], by.x = "JobSatisfaction", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "JobSatisfactionText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "PerformanceRating", 2:3], by.x = "PerformanceRating", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "PerformanceRatingText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "RelationshipSatisfaction", 2:3], by.x = "RelationshipSatisfaction", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "RelationshipSatisfactionText"

attrition_df <- merge(x = attrition_df, y = attrition_ref_data_df[attrition_ref_data_df$column == "WorkLifeBalance", 2:3], by.x = "WorkLifeBalance", by.y = "code", all.x = TRUE, all.y = FALSE)
col_count <- ncol(attrition_df)
colnames(attrition_df)[col_count] <- "WorkLifeBalanceText"

attrition_df <- attrition_df %>%
        mutate(censor = ifelse(Attrition == "Yes", 1, 0))

attrition_df <- as.data.frame(unclass(attrition_df))

attrition_data_df <- attrition_df[ , c(16:42, 8:14, 43)]
attrition_data_df$censor <- factor(attrition_data_df$censor)
attrition_df$Attrition <- factor(attrition_df$Attrition, levels = c("Yes", "No"))

```

#### Exploratory analysis

```{r echo = FALSE}

term_dist_viz <- ggplot(attrition_df, aes(x = Attrition, fill = Attrition)) +
        geom_bar() +
        geom_text(stat = "count", aes(label = ..count..), hjust = -0.5) +
        scale_fill_manual(values = c("#ED8800", "#5378b5")) +
        labs(title = "Employee count by attrition category", subtitle = "(no = currently active; yes = terminated)", x = "", y = "") +
        coord_flip() +
        theme(panel.background = element_blank(),
              axis.line.x = element_blank(),
              axis.line.y = element_blank(),
              axis.ticks.x = element_blank(),
              axis.ticks.y = element_blank(),
              axis.text.x = element_blank(),
              legend.position = "none")

renderPlot({
  term_dist_viz
})

```

Data exploration

```{r echo = FALSE}

selectInput(inputId = "explore_dim", label = "Choose the variable that you would like to explore:",
            choices = c("Gender", "Department", "Education", "Education Field", "Business Travel", "Overtime", "Stock Options Level",
                        "Job Satisfaction", "Work/Life Balance"), selected = "Gender")

dim1_select <- reactive({
        switch(input$explore_dim,
               "Gender" = attrition_df$Gender,
               "Department" = attrition_df$Department,
               "Education" = attrition_df$EducationText,
               "Education Field" = attrition_df$EducationField,
               "Business Travel" = attrition_df$BusinessTravel,
               "Overtime" = attrition_df$OverTime,
               "Stock Options Level" = attrition_df$StockOptionLevel,
               "Job Satisfaction" = attrition_df$JobSatisfactionText,
               "Work/Life Balance" = attrition_df$WorkLifeBalanceText)
        })

renderPlot({
        
        explore_dist_viz <- ggplot(attrition_df, aes(x = dim1_select())) +
                geom_bar(fill = "#8f8f8f") +
                geom_text(stat = "count", aes(label = ..count..), hjust = 1.5, color = "#ffffff") +
                labs(title = paste0("Employee count distribution"), x = "", y = "") +
                coord_flip() +
                theme(panel.background = element_blank(),
                      axis.line.x = element_blank(),
                      axis.line.y = element_blank(),
                      axis.ticks.x = element_blank(),
                      axis.ticks.y = element_blank(),
                      axis.text.x = element_blank(),
                      legend.position = "none")
        
        explore2_dist_viz <- ggplot(attrition_df, aes(x = dim1_select(), y = EmployeeCount, fill = Attrition)) +
                geom_bar(position = "fill", stat = "identity") +
                labs(title = paste0("Active versus terminated distribution"), x = "", y = "") +
                scale_fill_manual(values = c("#ED8800", "#5378b5")) +
                scale_y_continuous(label = percent_format()) +
                coord_flip() +
                theme(panel.background = element_blank(),
                      axis.line.y = element_blank(),
                      axis.ticks.y = element_blank())
        
        explore2_dist_viz
        
        grid.arrange(explore_dist_viz, explore2_dist_viz, ncol = 2)
        
})

```

#### Survival Analysis

```{r echo = FALSE}

surv_object <- Surv(time = attrition_df$YearsAtCompany, event = attrition_df$censor)

fit1 <- survfit(surv_object ~ 1, data = attrition_df)

model_df <- fortify(fit1)

model_top20_df <- model_df %>%
        arrange(time) %>%
        mutate(avg_probability_decrease = lag(surv) - surv) %>%
        filter(time <= 20 & time > 0)

#ggsurvplot(fit1, data = attrition_df, pval = TRUE, risk.table = TRUE, surv.median.line = "hv")

survival_viz <- ggplot(model_df, aes(x = time, y = surv)) +
        geom_line(size = 1.25) +
        scale_y_continuous(label = percent_format()) +
        labs(title = "Retention probability analysis", subtitle = "", x = "years at company", y = "retention probability") +
        theme_minimal()

risk_viz <- ggplot(model_top20_df, aes(x = reorder(time, desc(time)), y = avg_probability_decrease)) +
        geom_bar(stat = "identity") +
        scale_y_continuous(label = percent_format()) +
        coord_flip() +
        labs(title = "Increase in retention risk for each year of service", subtitle = "", x = "tenure (years)", y = "average retention risk increase") +
        theme_minimal()

renderPlot({
  grid.arrange(survival_viz, risk_viz, ncol = 2)
})

```


```{r echo = FALSE}

attrition_df <- attrition_df %>%
        mutate(censor = ifelse(Attrition == "Yes", 1, 0))

selectInput(inputId = "survival_slice", label = "Choose the dimension by which you'd like to slice the retention analysis:", 
  choices = c("Gender", "Department", "Education", "Education Field", "Business Travel", "Overtime", "Stock Options Level",
              "Job Satisfaction", "Work/Life Balance"), selected = "Gender")

dimension_choice <- reactive({
        switch(input$survival_slice,
               "Gender" = attrition_df$Gender,
               "Department" = attrition_df$Department,
               "Education" = attrition_df$EducationText,
               "Education Field" = attrition_df$EducationField,
               "Business Travel" = attrition_df$BusinessTravel,
               "Overtime" = attrition_df$OverTime,
               "Stock Options Level" = attrition_df$StockOptionLevel,
               "Job Satisfaction" = attrition_df$JobSatisfactionText,
               "Work/Life Balance" = attrition_df$WorkLifeBalanceText)
        })

renderPlot({
        
        fit2 <- survfit(surv_object ~ dimension_choice(), data = attrition_df)
        
        model2_df <- fortify(fit2)
        
        model2_top20_df <- model2_df %>%
                arrange(strata, time) %>%
                mutate(avg_probability_decrease = lag(surv) - surv) %>%
                filter(time <= 20 & time > 0)
        
        survival2_viz <- ggplot(model2_df, aes(x = time, y = surv, color = strata)) +
                geom_line(size = 1.25) +
                scale_y_continuous(label = percent_format()) +
                labs(title = "Retention probability analysis", subtitle = "", x = "years at company", y = "retention probability") +
                theme_minimal()
        
        risk2_viz <- ggplot(model2_top20_df, aes(x = reorder(time, desc(time)), y = avg_probability_decrease)) +
                geom_bar(stat = "identity") +
                scale_y_continuous(label = percent_format()) +
                facet_wrap(~strata) +
                coord_flip() +
                labs(title = "Increase in retention risk for each year of service", subtitle = "", x = "tenure (years)", y = "average retention risk increase") +
                theme_minimal()
        
        grid.arrange(survival2_viz, risk2_viz, ncol = 2)
        
        })

```

#### Driver Analysis

Important variables

```{r echo = FALSE, message=FALSE, warning=FALSE}

driver_data_df <- attrition_data_df

# Remove StandardHours and Over18 since there is no variation and remove Attrition since it is redundant
driver_data_df <- driver_data_df[ , -c(10, 13, 29)]

colnames(driver_data_df)[32] <- "terminated"

model_driver_rf <- randomForest(terminated ~ ., data = driver_data_df[, -1])
driver_imp_rf <- varImp(model_driver_rf)
driver_imp_rf$driver <- row.names(driver_imp_rf)
driver_imp_rf <- driver_imp_rf[ , c(2, 1)] %>%
        arrange(desc(Overall))

renderPlot({
        
        variable_viz <- ggplot(driver_imp_rf, aes(x=reorder(driver, Overall), y = Overall)) +
                geom_point() +
                geom_segment(aes(x=driver,xend=driver,y=0,yend=Overall)) +
                labs(title = "Variable importance", subtitle = "(generated by random forest model)", x = "", y = "mean decrease in Gini") +
                theme(panel.background = element_blank(),
                      panel.grid.major.x = element_line(color = "#dee2e8")) +
                coord_flip()
        
        variable_viz
        
        })

```

Key drivers

```{r echo = FALSE, message=FALSE, warning=FALSE}

# Set Gini index cutoff

numericInput(inputId = "gini_cutoff", label = "Choose the Gini cut-off point below which you want to exclude variables:",
            min = min(driver_imp_rf$Overall), max = max(driver_imp_rf$Overall), value = max(driver_imp_rf$Overall))

renderTable({
        
        options(scipen = 999)
        
        variables_chosen <- driver_imp_rf %>% filter(Overall >= input$gini_cutoff) %>% dplyr::select(driver)
        
        variables_chosen <- as.vector(variables_chosen$driver)
        
        driver_data_f_df <- driver_data_df %>% dplyr::select(EmployeeNumber, variables_chosen, terminated)
        
        model_driver_lr <- glm(terminated ~ ., data = driver_data_f_df[, -1], family = "binomial")
        
        model_driver_lr_df <- tidy(model_driver_lr)
        
        model_driver_lr_df %>% gt()
        
})


```



# Machine Learning and Predictive Modeling

### Data Collection

```{r echo = FALSE, message=FALSE, warning=FALSE}

modeling_data_df <- attrition_data_df

# Remove StandardHours and Over18 since there is no variation and remove Attrition since it is redundant
modeling_data_df <- modeling_data_df[ , -c(10, 13, 29)]

colnames(modeling_data_df)[32] <- "terminated"

```


### Data Preparation

Split the data into test and training datasets. The training dataset is used to build the model and the test dataset is used to evaluate the accuracy and precision of the model.

```{r echo = FALSE, message=FALSE, warning=FALSE}

set.seed(1)

# Split the data into training and test set
training_samples_df <- modeling_data_df$EmployeeNumber %>% 
  createDataPartition(p = 0.8, list = FALSE)

train_data_df  <- modeling_data_df[training_samples_df, ]
test_data_df <- modeling_data_df[-training_samples_df, ]

```



### Build and Train the Models - Naive Bayes, Decision Trees and Logistic Regression

Naive Bayes classifier

```{r echo = FALSE, message=FALSE, warning=FALSE}

set.seed(1)

# Fit the models
model_nb <- NaiveBayes(terminated ~ ., data = train_data_df[, -1])
model_lr <- glm(terminated ~ ., data = train_data_df[, -1], family = "binomial")
model_rf <- randomForest(terminated ~ ., data = train_data_df[, -1])
model_rt <- rpart(terminated ~ ., data = train_data_df[, -1])

# Make predictions
predictions_nb <- model_nb %>% predict(test_data_df)
predictions_lr_resp <- model_lr %>% predict(test_data_df, type = "response")
predictions_lr <- factor(ifelse(predictions_lr_resp > 0.5, 1, 0), levels = c(0, 1))
predictions_rf <- model_rf %>% predict(test_data_df)
predictions_rt <- model_rt %>% predict(test_data_df, type = "class")

# Model accuracies
model_build_accuracy_nb <- mean(predictions_nb$class == test_data_df$terminated)
model_build_accuracy_lr <- mean(predictions_lr == test_data_df$terminated)
model_build_accuracy_rf <- mean(predictions_rf == test_data_df$terminated)
model_build_accuracy_rt <- mean(predictions_rt == test_data_df$terminated)

# Confusion matrices
model_build_cm_nb <- confusionMatrix(predictions_nb$class, test_data_df$terminated)
model_build_cm_lr <- confusionMatrix(predictions_lr, test_data_df$terminated)
model_build_cm_rf <- confusionMatrix(predictions_rf, test_data_df$terminated)
model_build_cm_rt <- confusionMatrix(predictions_rt, test_data_df$terminated)

# Model performance metrics
# F-Score: (2 * Precision * Recall)/(Precision + Recall) harmonic mean of precision and recall
perf_metrics_nb_untrained <- as.data.frame(c(model_build_cm_nb$overall, model_build_cm_nb$byClass))
perf_metrics_lr_untrained <- as.data.frame(c(model_build_cm_lr$overall, model_build_cm_lr$byClass))
perf_metrics_rf_untrained <- as.data.frame(c(model_build_cm_rf$overall, model_build_cm_rf$byClass))
perf_metrics_rt_untrained <- as.data.frame(c(model_build_cm_rt$overall, model_build_cm_rt$byClass))

metric_labels <- row.names(perf_metrics_nb_untrained)

perf_metrics_untrained_df <- bind_cols(perf_metrics_nb_untrained, perf_metrics_lr_untrained, perf_metrics_rf_untrained,
                                       perf_metrics_rt_untrained)
names(perf_metrics_untrained_df) <- c("naive_bayes_untrained", "logistic_regression_untrained",
                                      "random_forest_untrained", "regression_tree_untrained")
perf_metrics_untrained_df$metric <- metric_labels

perf_metrics_untrained_df <- perf_metrics_untrained_df[ , c(5, 2, 1, 3:4)]

perf_metrics_untrained_df %>%
        gt() %>%
        fmt_percent(c(2:5), decimals = 1)

```



### Evaluate Model Performance



```{r echo = FALSE, message=FALSE, warning=FALSE}

set.seed(1)

# Build the models
model_test_nb_df <- train(terminated ~ ., data = train_data_df[, -1], method = "nb",
                          trControl = trainControl("cv", number = 10))
model_test_lr_df <- train(terminated ~ ., data = train_data_df[, -1], method = "glm", family = "binomial",
                          trControl = trainControl("cv", number = 10))
model_test_rf_df <- train(terminated ~ ., data = train_data_df[, -1], method = "rf",
                          trControl = trainControl("cv", number = 10))
model_test_rt_df <- train(terminated ~ ., data = train_data_df[, -1], method = "rpart",
                          trControl = trainControl("cv", number = 10))

# Make predictions
predicted_classes_nb <- model_test_nb_df %>% predict(test_data_df)
predicted_classes_lr <- model_test_lr_df %>% predict(test_data_df)
predicted_classes_rf <- model_test_rf_df %>% predict(test_data_df)
predicted_classes_rt <- model_test_rt_df %>% predict(test_data_df)

# Calculate the confusion matrices
model_trained_cm_nb <- confusionMatrix(predicted_classes_nb, test_data_df$terminated)
model_trained_cm_lr <- confusionMatrix(predicted_classes_lr, test_data_df$terminated)
model_trained_cm_rf <- confusionMatrix(predicted_classes_rf, test_data_df$terminated)
model_trained_cm_rt <- confusionMatrix(predicted_classes_rt, test_data_df$terminated)

# Model validation metrics
# F-Score: (2 * Precision * Recall)/(Precision + Recall) harmonic mean of precision and recall
perf_metrics_nb_trained <- as.data.frame(c(model_trained_cm_nb$overall, model_trained_cm_nb$byClass))
perf_metrics_lr_trained <- as.data.frame(c(model_trained_cm_lr$overall, model_trained_cm_lr$byClass))
perf_metrics_rf_trained <- as.data.frame(c(model_trained_cm_rf$overall, model_trained_cm_rf$byClass))
perf_metrics_rt_trained <- as.data.frame(c(model_trained_cm_rt$overall, model_trained_cm_rt$byClass))

metric_labels <- row.names(perf_metrics_nb_trained)

perf_metrics_trained_df <- bind_cols(perf_metrics_nb_trained, perf_metrics_lr_trained, perf_metrics_rf_trained, perf_metrics_rt_trained)
names(perf_metrics_trained_df) <- c("naive_bayes_trained", "logistic_regression_trained", "random_forest_trained", "regression_tree_trained")
perf_metrics_trained_df$metric <- metric_labels

perf_metrics_trained_df <- perf_metrics_trained_df[ , c(5, 2, 1, 3:4)]

perf_metrics_trained_df %>%
        gt() %>%
        fmt_percent(c(2:5), decimals = 1)

```


### Improve Model Performance


```{r echo = FALSE, message=FALSE, warning=FALSE}

set.seed(1)

options(scipen = 999)

feature_select_lr <- varImp(model_test_lr_df)$importance

```



```{r echo = FALSE, message=FALSE, warning=FALSE}

set.seed(1)

options(scipen = 999)

varImp(model_test_lr_df)
varImp(model_test_nb_df)
varImp(model_test_rf_df)
varImp(model_test_rt_df)

model_input_data_df <- attrition_data_df[ , c(2:9, 11:28, 30:35)]

fit_rf <- randomForest(censor ~ ., data = model_input_data_df)

# Create an importance based on mean decreasing gini
gini_df <- as.data.frame(importance(fit_rf))

```



# Conclusion


# References and Resources
Following are the online articles and resources I used in compiling this analysis:

  * **Employee attrition articles and references >>**
    * *Extended Tutorial: How to Predict Employee Turnover >>* https://www.hranalytics101.com/extended-tutorial-how-to-predict-employee-turnover/
  * **Sample attrition data posted by IBM >>** https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/
  * **Interactive Shiny documents >>**
    * https://bookdown.org/yihui/rmarkdown/shiny-documents.html
    * https://rmarkdown.rstudio.com/lesson-14.html
  * **Survival analysis >>** https://www.datacamp.com/community/tutorials/survival-analysis-R
  * **Feature selection >>** 
    * https://www.r-bloggers.com/feature-selection-using-the-caret-package/
    * http://dataaspirant.com/2018/01/15/feature-selection-techniques-r/
    * https://www.r-bloggers.com/variable-importance-plot-and-variable-selection/
  * **Machine Learning/Predictive Modeling >>** *Machine Learning with R*, (2013) by Brett Lantz 
    * **Naive Bayes >> ** 
      * https://www.r-bloggers.com/understanding-naive-bayes-classifier-using-r/
      * *Naive Bayes Classifier Essentials >>* http://www.sthda.com/english/articles/36-classification-methods-essentials/145-naive-bayes-classifier-essentials/
    * **Logistic regression >>** 
      * https://www.datacamp.com/community/tutorials/logistic-regression-R
      * https://www.r-bloggers.com/evaluating-logistic-regression-models/
  * **Online tools >>** 
    * https://www.google.com/search?q=color+picker
  * **Technical troubleshooting >>** https://stackoverflow.com/
    * https://stackoverflow.com/questions/8499361/easy-way-of-counting-precision-recall-and-f1-score-in-r
    * **gtable >>** https://www.rdocumentation.org/packages/gt/versions/0.1.0




